{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69c8bf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for Reddit API data collection\n",
    "import praw  # For making HTTP requests to Reddit API\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import re\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "import string\n",
    "import os\n",
    "from pysentimiento.preprocessing import preprocess_tweet\n",
    "from pysentimiento import create_analyzer\n",
    "\n",
    "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0825fc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reddit_instance():\n",
    "    reddit = praw.Reddit(\n",
    "        client_id = 'LDPs59MIc82xLXPftHQ3Sw',\n",
    "        client_secret = \"uZSSG2q3ueggUkUReIMzaKe8xnC6iw\",\n",
    "        password = 'Linyihao.041228',\n",
    "        user_agent = \"Comment Extraction (by SaltyfishQAQ)\",\n",
    "        username = 'Few-Strength-2343'\n",
    "    )\n",
    "    \n",
    "    return reddit\n",
    "\n",
    "\n",
    "def get_monthly_top_posts(reddit, subreddit_name, limit=10, time_filter='month'):\n",
    "    count = 0\n",
    "    # Fetch the subreddit\n",
    "    subreddit = reddit.subreddit(subreddit_name)\n",
    "    \n",
    "    master_df = []\n",
    "    all_submissions = []\n",
    "    for submission in subreddit.top(time_filter=time_filter, limit=limit):\n",
    "        single_submission = []\n",
    "        # Collect submission information\n",
    "        submission_info = {\n",
    "            \"submission_title\": submission.title,\n",
    "            \"author\": submission.author.name if submission.author else \"deleted\",\n",
    "            \"id\": submission.id,\n",
    "            \"url\": submission.url,\n",
    "            \"created_utc\": submission.created_utc,\n",
    "            \"body\": submission.selftext,  \n",
    "            \"type\": \"submission\",\n",
    "            \"score\": submission.score\n",
    "        }\n",
    "        master_df.append(submission_info)\n",
    "        single_submission.append(submission_info)\n",
    "        \n",
    "        if submission.num_comments == 0:\n",
    "            continue\n",
    "        else:\n",
    "            submission.comments.replace_more(limit=None)\n",
    "            for comment in submission.comments.list():\n",
    "                # Skip MoreComments objects if any remain\n",
    "                if isinstance(comment, praw.models.MoreComments):\n",
    "                    continue\n",
    "                \n",
    "                comment_info = {\n",
    "                    \"submission_title\": submission.title,\n",
    "                    \"author\": comment.author.name if comment.author else \"deleted\",\n",
    "                    \"id\": comment.id,\n",
    "                    \"url\": f\"https://www.reddit.com{comment.permalink}\",\n",
    "                    \"created_utc\": comment.created_utc,\n",
    "                    \"body\": comment.body,\n",
    "                    \"type\": \"comment\",\n",
    "                    \"score\": comment.score\n",
    "                }\n",
    "                master_df.append(comment_info)\n",
    "                single_submission.append(comment_info)\n",
    "        \n",
    "        all_submissions.append(single_submission)\n",
    "        count += 1\n",
    "        print(f\"Processed submission {count}/{limit}: {submission.title}\")\n",
    "        \n",
    "    master_df = pd.DataFrame(master_df).reset_index(drop=True)\n",
    "        \n",
    "    return master_df, all_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09410031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed submission 1/100: Bring back international students$$$$$$$$$$$$$$$$\n",
      "Processed submission 2/100: these types of questions are actually so dumb what\n",
      "Processed submission 3/100: Unexpected perks of Toronto Public Library for college/university students\n",
      "Processed submission 4/100: Manifestation thread for a good academic year üôèüôèüôè\n",
      "Processed submission 5/100: Asked to leave at cafeteria because I have a waterbottle on my bag\n",
      "Processed submission 6/100: quick walk to uoft scarborough from utsg before classes start\n",
      "Processed submission 7/100: Was looking for storage options and saw this lmao wtffff\n",
      "Processed submission 8/100: Beware of Religious cults on campus that are actively recruiting\n",
      "Processed submission 9/100: Dr Ruth Marshall is now \"on leave\" following her comments in response to the murder of Charlie Kirk; university actively investigating the situation\n",
      "Processed submission 10/100: WELCOME BACK TO ANOTHER EPISODE OF STFU IN CLASS FOR THE LORD‚ÄôS SAKE\n",
      "Processed submission 11/100: My honest meme recreation of being a first year student so far\n",
      "Processed submission 12/100: As we begin a new academic year, let's all just be thankful that we live in this country\n",
      "Processed submission 13/100: I made uoft orientation list for 1st year students (took me straight 12 hours)\n",
      "Processed submission 14/100: I have ruined my GPA and brought shame upon my family\n",
      "Processed submission 15/100: Oak House construction delayed one day before move in; absolutely enraged\n",
      "Processed submission 16/100: High school is a Push Based System, University is a Pull Based System\n",
      "Processed submission 17/100: PLEASE JUST WEAR A MASK IF YOUR SICK plsssssssssss\n",
      "Processed submission 18/100: Feeling real down and horrible about the first week of uni\n",
      "Processed submission 19/100: Being financially unstable at UofT and other universities\n",
      "Processed submission 20/100: WHY DOES NO ONE TALK IN CLASSES | YA'LL HAVE IMMUNITY??\n",
      "Processed submission 21/100: College and St George Bus Stop strange signs, does anyone know what they mean?\n",
      "Processed submission 22/100: University College food is quite saddening, how are people expected to accept this??\n",
      "Processed submission 23/100: Can yall NOT come to uni when yall are sick ?? Im not trying to get sick AGAIN\n",
      "Processed submission 24/100: Students please Watch out for professional thieves\n",
      "Processed submission 25/100: We should create \"The Almost Missed Opportunities\" (or something) club for upper year students who feel like they missed out on a lot of the extra curricular/social opportunities at U of T so far\n",
      "Processed submission 26/100: To the two people talking in Gerstein from 7:00 PM onwards on Wednesday\n",
      "Processed submission 27/100: will i get in trouble if i tell them they will never see the gates of heaven\n",
      "Processed submission 28/100: and completed his undergrad here at uoft wooooooo\n",
      "Processed submission 29/100: how do people have a life at this school genuinely\n",
      "Processed submission 30/100: What is your favourite major architecture style around campus? Classical, Gothic, Romanesque or Brutalist?\n",
      "Processed submission 31/100: long rant about my personal experience with u of t counselling services\n",
      "Processed submission 32/100: Starting this Academic Year, Robarts Library will be Banning Food on Some Floors Like 9 to 13 to Preserve the Library Materials\n",
      "Processed submission 33/100: School starts 1 week from now ain‚Äôt that so crazy\n",
      "Processed submission 34/100: I found this inside Robarts, does anyone know what this is about? Any idea where the next clue could be?\n",
      "Processed submission 35/100: Want to start a Losers club for those who have already graduated\n",
      "Processed submission 36/100: DONT FALL FOR THIS PHISHING EMAIL SCAM, NOT REAL and not from UofT\n",
      "Processed submission 37/100: The horrifying Pikachu mascot that got arrested by campus security last year has a meet and greet on Tuesday??\n",
      "Processed submission 38/100: How do I make it across in 10 minutes? I have back to back classes (from MB to LA)\n",
      "Processed submission 39/100: cri215 sat us on the floor of a high school gym üòçüòçüòç\n",
      "Processed submission 40/100: New scam going around?? Is this happening to anyone else?\n",
      "Processed submission 41/100: What are your favourite spots to enjoy the fall colours around St George campus?\n",
      "Processed submission 42/100: No menstrual pads at Robarts (character filler and again)\n",
      "Processed submission 43/100: Shared restrooms are a public place, NOT your home\n",
      "Processed submission 44/100: UofT should have a student contest for a TCard redesign\n",
      "Processed submission 45/100: any other first years scared af for tmmrw üò≠üò≠üò≠üò≠üò≠üò≠üò≠üò≠üò≠\n",
      "Processed submission 46/100: get your free egg mcmuffins at the northrop frye branch\n",
      "Processed submission 47/100: LONELY mfs with ZERO friends, what are you ü´µ doing this weekend?\n",
      "Processed submission 48/100: What the hell is going on with New College residence quality control??\n",
      "Processed submission 49/100: Is the break dancing in myhal normal or do i just happen to be there when they're there\n",
      "Processed submission 50/100: What is up with people dyed purple around the campus?\n",
      "Processed submission 51/100: I analyzed over 1000+ clubs to find the most interesting events at UofT (+ Update on Orientation events)\n",
      "Processed submission 52/100: Any shortcuts to walk this in 10 minutes pleaseee üò¢üôè\n",
      "Processed submission 53/100: Part time student association was full time out of office this summer\n",
      "Processed submission 54/100: Friendly reminder to drop your courses you don‚Äôt need it ‚ù§Ô∏è\n",
      "Processed submission 55/100: Anyone at UTM right now, please be careful, this is absolutely disgusting\n",
      "Processed submission 56/100: Do we seriously have to pay hundreds of dollars to buy textbooks so that we can do weekly quizzes?\n",
      "Processed submission 57/100: i think im crashing out way to early into the school year\n",
      "Processed submission 58/100: everyone is scared to make connections and friends\n",
      "Processed submission 59/100: If you have a course taught by Pitt and Baumgartner, do NOT take Baumgartner\n",
      "Processed submission 60/100: Spotted someone spitting 5 times (and counting) inside MP\n",
      "Processed submission 61/100: can someone please explain how to go to the gym at uoft\n",
      "Processed submission 62/100: How do I become the most unhelpful person on the entire Earth?\n",
      "Processed submission 63/100: Feeling like AI detectors are trolling us and making me paranoid\n",
      "Processed submission 64/100: Yes, that email is a scam: a redux from last year's post to avoid being grifted\n",
      "Processed submission 65/100: i'm a 4th year in a specialist program and got into a half credit of 400 levels courses\n",
      "Processed submission 66/100: 23F, first year humanities, wondering how to meet people my age\n",
      "Processed submission 67/100: how do I get a prof to know me enough to write a letter of rec?\n",
      "Processed submission 68/100: Found Sara K*****‚Äôs TCard at GradFest today (last two digits are 41)\n",
      "Processed submission 69/100: Best florist for UofT girlfriend (Downtown/Scarborough)?\n",
      "Processed submission 70/100: 2nd year Archaeology Specialist fall sched am I cooked?\n",
      "Processed submission 71/100: How are ya‚Äôll getting to classes across campus in 10min??\n",
      "Processed submission 72/100: In my fourth year and have no friends lol I crave social interaction\n",
      "Processed submission 73/100: my lecture note dump for math courses (+ chem and stats)\n",
      "Processed submission 74/100: Tap to enter instead of swipe at Gerstein Library\n",
      "Processed submission 75/100: Best salad places near uoft please i‚Äôm begging so hard\n",
      "Processed submission 76/100: Do UofT medical students just stay inside the Medical Science building or do they venture out into buildings like Bahen and McLennan?\n",
      "Processed submission 77/100: MAT157 is eating me alive, please give me advice on how not to die\n",
      "Processed submission 78/100: Why does Robarts look like Luthorcorp? Malicious things going on\n",
      "Processed submission 79/100: Does anybody know where all 8 of the UTM slenderman pages are?\n",
      "Processed submission 80/100: Genuinely how the hell do you talk to people because it‚Äôs confusing me\n",
      "Processed submission 81/100: istg these scams become more and more stupid day by day\n",
      "Processed submission 82/100: the eng kids seem to have so much fun together :)))\n",
      "Processed submission 83/100: New semester is coming, what‚Äôs ur plan and wish list ?\n",
      "Processed submission 84/100: Is uoft called uoftears for the amount of work load or is there more to it?\n",
      "Processed submission 85/100: I made a website that has every event happening on campus\n",
      "Processed submission 86/100: Can I survive engineering at U of T without a laptop?\n",
      "Processed submission 87/100: reminder to drop your classes for the fall semester by tomorrow (sept 15th)\n",
      "Processed submission 88/100: I hate this course format raaaggghghhhhhhhhhhhgahhgfhaghsd\n",
      "Processed submission 89/100: flight cancelled, missing first day of orientation üò≠\n",
      "Processed submission 90/100: does it get easier? first year fighting demons rn help\n",
      "Processed submission 91/100: Would it be weird to do this (first year advice plz)\n",
      "Processed submission 92/100: Has anyone considered dropping out and just getting a regular job?\n",
      "Processed submission 93/100: What can commuter students do to be more socially open?\n",
      "Processed submission 94/100: Stolen Pink bag in Women's locker room at Athletic Centre, St George Campus\n",
      "Processed submission 95/100: does anyone know what happened at robarts commons this morning\n",
      "Processed submission 96/100: Lift of Suspension Denied, What should I do to my uni life\n",
      "Processed submission 97/100: Offer was withdrawn, out of my control, highschool made mistakes\n",
      "Processed submission 98/100: Hundreds of U of T students caught off guard by delay in opening of new student residence\n",
      "Processed submission 99/100: Two Offers (Toronto vs Waterloo) Is Toronto worth the extra cost?\n"
     ]
    }
   ],
   "source": [
    "reddit = get_reddit_instance()\n",
    "master_df, all_submissions = get_monthly_top_posts(reddit, 'UofT', limit=100, time_filter='month')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29739bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('top_100_reddits_sep20th')\n",
    "for i, submission in enumerate(all_submissions):\n",
    "    submission_df = pd.DataFrame(submission).reset_index(drop=True)\n",
    "    submission_df.to_csv(f'top_100_reddits_sep20th/submission_{i}.csv', index=False)\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "606a6547",
   "metadata": {},
   "outputs": [],
   "source": [
    "master_df.to_csv(\"top_100_reddits_sep20th.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e9c81cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2593, 8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d23247",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning step\n",
    "\n",
    "# for the selftexts: \n",
    "# lowercase all texts\n",
    "# Delete spaces in front of and after the texts, keep the spaces in between\n",
    "# Delete all special characters except for spaces\n",
    "# Remove all error reading form the api. eg. error languange\n",
    "\n",
    "# irelevant comments:\n",
    "# Delete all comments that are not related to UofT: url not start from: https://www.reddit.com/r/UofT/comments/\n",
    "\n",
    "# Time conveting:\n",
    "# Convert time format in created_utc to readable format(20XX-XX-XX)\n",
    "# utc format: scientific format for secs; 1.752931e+09 = 1,757,931,000 sec since Jan 1, 1970 UTC; \n",
    "# represent time stamp 2025-07-23 05:03:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62763e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove all special characters from the text except for letters, numbers, and spaces.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def convert_emoji_to_text(text, emoji_wrapper=\"emoji\"):\n",
    "    \"\"\"\n",
    "    Converts emoji in the text to descriptive text.\n",
    "    \n",
    "    This function uses emoji.demojize() to replace any emoji with their\n",
    "    corresponding text (e.g., \"üòÑ\" becomes \":smile:\"). It then wraps the\n",
    "    emoji description using the provided emoji_wrapper.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The original text containing emoji.\n",
    "        emoji_wrapper (str): A string that will be used to wrap the emoji text.\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with emojis converted to wrapped text.\n",
    "    \"\"\"\n",
    "    # Convert emojis to descriptive text (e.g., :smile:)\n",
    "    demojized = emoji.demojize(text)\n",
    "    # Define a wrapper string (e.g., \" emoji \")\n",
    "    wrapper = f\" {emoji_wrapper} \".replace(\"  \", \" \")\n",
    "    # Replace the demojized emoji pattern :emoji_name: with the wrapped emoji_name\n",
    "    # For example, \":smile:\" becomes \" emoji smile emoji \"\n",
    "    result = re.sub(r':([^:\\s]+):', lambda m: wrapper + m.group(1) + wrapper, demojized)\n",
    "    return result\n",
    "\n",
    "def is_valid_word(word):\n",
    "    \"\"\"\n",
    "    Returns True if the word is either:\n",
    "    - Composed solely of punctuation\n",
    "    - Contains at least one English letter (a-z or A-Z)\n",
    "    \"\"\"\n",
    "    # Keep if word is only punctuation\n",
    "    if all(ch in string.punctuation for ch in word):\n",
    "        return True\n",
    "    # Keep if the word contains at least one ASCII letter\n",
    "    if re.search(r'[a-zA-Z]', word):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_non_english(text):\n",
    "    \"\"\"\n",
    "    Splits the text into words and filters out any word that is not an English word,\n",
    "    an emoji, or punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if is_valid_word(word)]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 4. Convert created_utc to readable date format (YYYY-MM-DD)\n",
    "def convert_utc_to_readable(utc_timestamp):\n",
    "    \"\"\"\n",
    "    Convert UTC timestamp to a readable date format (YYYY-MM-DD).\n",
    "    \n",
    "    Args:\n",
    "        utc_timestamp (float): UTC timestamp in seconds since epoch.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted date string or empty string if input is NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Handle NaN values\n",
    "        if pd.isna(utc_timestamp):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to datetime object\n",
    "        dt = datetime.fromtimestamp(utc_timestamp)\n",
    "        \n",
    "        # Format as YYYY-MM-DD HH:MM:SS\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    except (ValueError, TypeError, OSError):\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bed9bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Keep only posts with URLs starting with UofT comments\n",
    "master_df = master_df[master_df['url'].str.startswith('https://www.reddit.com/r/UofT/comments/')].reset_index(drop=True).copy()\n",
    "\n",
    "# 2. Clean selftext: lowercase, strip, remove special chars except spaces\n",
    "master_df['body'] = master_df['body'].str.lower()\n",
    "master_df['body'] = master_df['body'].str.strip()\n",
    "\n",
    "# 3. Remove special characters except spaces\n",
    "master_df['body'] = master_df['body'].apply(clean_text)\n",
    "\n",
    "# 4. Converts emoji in the text to descriptive text.\n",
    "master_df['body'] = master_df['body'].apply(convert_emoji_to_text)\n",
    "\n",
    "# 5. Filter out non-English words\n",
    "master_df['body'] = master_df['body'].apply(filter_non_english)\n",
    "\n",
    "# 6. Convert created_utc to readable date format\n",
    "master_df['created_utc'] = master_df['created_utc'].apply(convert_utc_to_readable)\n",
    "\n",
    "# 7. Preprocess the selftext column using pysentimiento\n",
    "# master_df['body'] = master_df['body'].apply(preprocess_tweet)\n",
    "\n",
    "master_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eac8e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(master_df, text_column='body'):\n",
    "    \"\"\"\n",
    "    Iterates through each text in the DataFrame, applies sentiment_analyzer,\n",
    "    and adds two columns: sentiment_prediction and sentiment_prob.\n",
    "    \n",
    "    Args:\n",
    "        master_df (pd.DataFrame): The DataFrame to analyze\n",
    "        text_column (str): The column containing text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added sentiment columns\n",
    "    \"\"\"\n",
    "    senti_pred = []\n",
    "    senti_prob = []\n",
    "    \n",
    "    for index, row in master_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        \n",
    "        if pd.isna(text) or text == \"\":\n",
    "            senti_pred.append(\"NEU\")\n",
    "            senti_prob.append({\"NEU\": 0.34, \"NEG\": 0.33, \"POS\": 0.33})\n",
    "        else:\n",
    "            result = sentiment_analyzer.predict(text)\n",
    "            senti_pred.append(result.output)\n",
    "            senti_prob.append(result.probas)\n",
    "    \n",
    "    master_df['sentiment_prediction'] = senti_pred\n",
    "    master_df['sentiment_prob'] = senti_prob\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all preprocessing steps to a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # 1. Keep only posts with URLs starting with UofT comments\n",
    "    processed_df = processed_df[processed_df['url'].str.startswith('https://www.reddit.com/r/UofT/comments/')].reset_index(drop=True).copy()\n",
    "    \n",
    "    # 2. Clean body text: lowercase, strip, remove special chars except spaces\n",
    "    processed_df['body'] = processed_df['body'].str.lower()\n",
    "    processed_df['body'] = processed_df['body'].str.strip()\n",
    "    \n",
    "    # 3. Remove special characters except spaces\n",
    "    processed_df['body'] = processed_df['body'].apply(clean_text)\n",
    "    \n",
    "    # 4. Converts emoji in the text to descriptive text\n",
    "    processed_df['body'] = processed_df['body'].apply(convert_emoji_to_text)\n",
    "    \n",
    "    # 5. Filter out non-English words\n",
    "    processed_df['body'] = processed_df['body'].apply(filter_non_english)\n",
    "    \n",
    "    # 6. Convert created_utc to readable date format\n",
    "    processed_df['created_utc'] = processed_df['created_utc'].apply(convert_utc_to_readable)\n",
    "    \n",
    "    # 7. Preprocess the body column using pysentimiento (optional - currently commented out)\n",
    "    # processed_df['body'] = processed_df['body'].apply(preprocess_tweet)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def process_all_monthly_submissions(folder_path='individual_submissions_month'):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the monthly submissions folder.\n",
    "    Apply preprocessing and sentiment analysis to each file.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all processed submissions\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    csv_files.sort()  # Sort to process in order\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    all_processed_data = []\n",
    "    \n",
    "    for i, filename in enumerate(csv_files):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file {i+1}/{len(csv_files)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  - Loaded {len(df)} rows\")\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            processed_df = preprocess_dataframe(df)\n",
    "            print(f\"  - After filtering: {len(processed_df)} rows\")\n",
    "            \n",
    "            # Apply sentiment analysis\n",
    "            if len(processed_df) > 0:\n",
    "                processed_df = sentiment_analysis(processed_df, text_column='body')\n",
    "                print(f\"  - Sentiment analysis completed\")\n",
    "                \n",
    "                # Add source file information\n",
    "                processed_df['source_file'] = filename\n",
    "                \n",
    "                all_processed_data.append(processed_df)\n",
    "            else:\n",
    "                print(f\"  - No data remaining after filtering\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all processed data\n",
    "    if all_processed_data:\n",
    "        combined_df = pd.concat(all_processed_data, ignore_index=True)\n",
    "        print(f\"\\nProcessing complete! Combined dataset has {len(combined_df)} rows\")\n",
    "        return combined_df\n",
    "    else:\n",
    "        print(\"\\nNo data was successfully processed\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# processed_monthly_data = process_all_monthly_submissions('individual_submissions_month')\n",
    "# processed_monthly_data.to_excel('processed_monthly_submissions_with_sentiment.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9038feb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process all monthly submission CSV files...\n",
      "Found 100 CSV files to process\n",
      "Processing file 1/100: submission_0.csv\n",
      "  - Loaded 59 rows\n",
      "  - After filtering: 58 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 2/100: submission_1.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 2/100: submission_1.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 3/100: submission_10.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 3/100: submission_10.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 4/100: submission_11.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 5/100: submission_12.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 27 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 4/100: submission_11.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 5/100: submission_12.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 27 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 6/100: submission_13.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 6/100: submission_13.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 7/100: submission_14.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 7/100: submission_14.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 8/100: submission_15.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 8/100: submission_15.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 9/100: submission_16.csv\n",
      "  - Loaded 39 rows\n",
      "  - After filtering: 38 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 9/100: submission_16.csv\n",
      "  - Loaded 39 rows\n",
      "  - After filtering: 38 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 10/100: submission_17.csv\n",
      "  - Loaded 45 rows\n",
      "  - After filtering: 45 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 10/100: submission_17.csv\n",
      "  - Loaded 45 rows\n",
      "  - After filtering: 45 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 11/100: submission_18.csv\n",
      "  - Loaded 23 rows\n",
      "  - After filtering: 23 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 11/100: submission_18.csv\n",
      "  - Loaded 23 rows\n",
      "  - After filtering: 23 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 12/100: submission_19.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 12/100: submission_19.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 13/100: submission_2.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 13/100: submission_2.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 14/100: submission_20.csv\n",
      "  - Loaded 25 rows\n",
      "  - After filtering: 25 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 14/100: submission_20.csv\n",
      "  - Loaded 25 rows\n",
      "  - After filtering: 25 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 15/100: submission_21.csv\n",
      "  - Loaded 28 rows\n",
      "  - After filtering: 27 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 15/100: submission_21.csv\n",
      "  - Loaded 28 rows\n",
      "  - After filtering: 27 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 16/100: submission_22.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 16/100: submission_22.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 17/100: submission_23.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 18/100: submission_24.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 17/100: submission_23.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 18/100: submission_24.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 19/100: submission_25.csv\n",
      "  - Loaded 50 rows\n",
      "  - After filtering: 49 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 19/100: submission_25.csv\n",
      "  - Loaded 50 rows\n",
      "  - After filtering: 49 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 20/100: submission_26.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 20/100: submission_26.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 21/100: submission_27.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 21/100: submission_27.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 22/100: submission_28.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 22/100: submission_28.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 23/100: submission_29.csv\n",
      "  - Loaded 24 rows\n",
      "  - After filtering: 24 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 23/100: submission_29.csv\n",
      "  - Loaded 24 rows\n",
      "  - After filtering: 24 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 24/100: submission_3.csv\n",
      "  - Loaded 93 rows\n",
      "  - After filtering: 93 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 24/100: submission_3.csv\n",
      "  - Loaded 93 rows\n",
      "  - After filtering: 93 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 25/100: submission_30.csv\n",
      "  - Loaded 31 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 25/100: submission_30.csv\n",
      "  - Loaded 31 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 26/100: submission_31.csv\n",
      "  - Loaded 48 rows\n",
      "  - After filtering: 48 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 26/100: submission_31.csv\n",
      "  - Loaded 48 rows\n",
      "  - After filtering: 48 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 27/100: submission_32.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 27/100: submission_32.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 28/100: submission_33.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 28/100: submission_33.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 29/100: submission_34.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 29/100: submission_34.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 30/100: submission_35.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 31/100: submission_36.csv\n",
      "  - Loaded 30 rows\n",
      "  - After filtering: 30 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 30/100: submission_35.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 31/100: submission_36.csv\n",
      "  - Loaded 30 rows\n",
      "  - After filtering: 30 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 32/100: submission_37.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 32/100: submission_37.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 33/100: submission_38.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 33/100: submission_38.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 34/100: submission_39.csv\n",
      "  - Loaded 49 rows\n",
      "  - After filtering: 49 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 34/100: submission_39.csv\n",
      "  - Loaded 49 rows\n",
      "  - After filtering: 49 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 35/100: submission_4.csv\n",
      "  - Loaded 71 rows\n",
      "  - After filtering: 71 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 35/100: submission_4.csv\n",
      "  - Loaded 71 rows\n",
      "  - After filtering: 71 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 36/100: submission_40.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 36/100: submission_40.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 37/100: submission_41.csv\n",
      "  - Loaded 38 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 37/100: submission_41.csv\n",
      "  - Loaded 38 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 38/100: submission_42.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 38/100: submission_42.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 39/100: submission_43.csv\n",
      "  - Loaded 59 rows\n",
      "  - After filtering: 59 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 39/100: submission_43.csv\n",
      "  - Loaded 59 rows\n",
      "  - After filtering: 59 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 40/100: submission_44.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 40/100: submission_44.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 41/100: submission_45.csv\n",
      "  - Loaded 42 rows\n",
      "  - After filtering: 42 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 41/100: submission_45.csv\n",
      "  - Loaded 42 rows\n",
      "  - After filtering: 42 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 42/100: submission_46.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 42/100: submission_46.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 43/100: submission_47.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 43/100: submission_47.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 44/100: submission_48.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 44/100: submission_48.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 45/100: submission_49.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 45/100: submission_49.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 46/100: submission_5.csv\n",
      "  - Loaded 55 rows\n",
      "  - After filtering: 55 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 46/100: submission_5.csv\n",
      "  - Loaded 55 rows\n",
      "  - After filtering: 55 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 47/100: submission_50.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 47/100: submission_50.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 48/100: submission_51.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 48/100: submission_51.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 49/100: submission_52.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 49/100: submission_52.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 50/100: submission_53.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 50/100: submission_53.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 51/100: submission_54.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 51/100: submission_54.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 52/100: submission_55.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 52/100: submission_55.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 53/100: submission_56.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 53/100: submission_56.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 54/100: submission_57.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 3 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 55/100: submission_58.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 54/100: submission_57.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 3 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 55/100: submission_58.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 56/100: submission_59.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 56/100: submission_59.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 57/100: submission_6.csv\n",
      "  - Loaded 52 rows\n",
      "  - After filtering: 51 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 57/100: submission_6.csv\n",
      "  - Loaded 52 rows\n",
      "  - After filtering: 51 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 58/100: submission_60.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 58/100: submission_60.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 59/100: submission_61.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 59/100: submission_61.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 60/100: submission_62.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 60/100: submission_62.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 61/100: submission_63.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 61/100: submission_63.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 62/100: submission_64.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 62/100: submission_64.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 63/100: submission_65.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 63/100: submission_65.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 64/100: submission_66.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 64/100: submission_66.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 65/100: submission_67.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 65/100: submission_67.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 66/100: submission_68.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 66/100: submission_68.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 67/100: submission_69.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 67/100: submission_69.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 68/100: submission_7.csv\n",
      "  - Loaded 35 rows\n",
      "  - After filtering: 35 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 68/100: submission_7.csv\n",
      "  - Loaded 35 rows\n",
      "  - After filtering: 35 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 69/100: submission_70.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 69/100: submission_70.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 70/100: submission_71.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 70/100: submission_71.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 71/100: submission_72.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 71/100: submission_72.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 72/100: submission_73.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 72/100: submission_73.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 73/100: submission_74.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 73/100: submission_74.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 74/100: submission_75.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 75/100: submission_76.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 74/100: submission_75.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 75/100: submission_76.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 76/100: submission_77.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 76/100: submission_77.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 77/100: submission_78.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 77/100: submission_78.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 78/100: submission_79.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 78/100: submission_79.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 79/100: submission_8.csv\n",
      "  - Loaded 37 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 79/100: submission_8.csv\n",
      "  - Loaded 37 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 80/100: submission_80.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 80/100: submission_80.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 81/100: submission_81.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 81/100: submission_81.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 82/100: submission_82.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 82/100: submission_82.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 83/100: submission_83.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 83/100: submission_83.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 84/100: submission_84.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 84/100: submission_84.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 85/100: submission_85.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 85/100: submission_85.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 86/100: submission_86.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 86/100: submission_86.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 87/100: submission_87.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 88/100: submission_88.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 87/100: submission_87.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 88/100: submission_88.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 89/100: submission_89.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 89/100: submission_89.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 90/100: submission_9.csv\n",
      "  - Loaded 38 rows\n",
      "  - After filtering: 38 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 90/100: submission_9.csv\n",
      "  - Loaded 38 rows\n",
      "  - After filtering: 38 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 91/100: submission_90.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 91/100: submission_90.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 92/100: submission_91.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 92/100: submission_91.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 93/100: submission_92.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 93/100: submission_92.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 94/100: submission_93.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 94/100: submission_93.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 95/100: submission_94.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 96/100: submission_95.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 95/100: submission_94.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 96/100: submission_95.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 97/100: submission_96.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 97/100: submission_96.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 98/100: submission_97.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 98/100: submission_97.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 99/100: submission_98.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 99/100: submission_98.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 100/100: submission_99.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 100/100: submission_99.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "\n",
      "Processing complete! Combined dataset has 2031 rows\n",
      "\n",
      "=== PROCESSING SUMMARY ===\n",
      "Total processed rows: 2031\n",
      "Unique submissions: 100\n",
      "Date range: 2025-07-07 23:13:42 to 2025-08-06 22:01:39\n",
      "\n",
      "=== SENTIMENT DISTRIBUTION ===\n",
      "sentiment_prediction\n",
      "NEU    1136\n",
      "NEG     487\n",
      "POS     408\n",
      "Name: count, dtype: int64\n",
      "Sentiment percentages:\n",
      "  NEU: 55.9%\n",
      "  NEG: 24.0%\n",
      "  POS: 20.1%\n",
      "  - Sentiment analysis completed\n",
      "\n",
      "Processing complete! Combined dataset has 2031 rows\n",
      "\n",
      "=== PROCESSING SUMMARY ===\n",
      "Total processed rows: 2031\n",
      "Unique submissions: 100\n",
      "Date range: 2025-07-07 23:13:42 to 2025-08-06 22:01:39\n",
      "\n",
      "=== SENTIMENT DISTRIBUTION ===\n",
      "sentiment_prediction\n",
      "NEU    1136\n",
      "NEG     487\n",
      "POS     408\n",
      "Name: count, dtype: int64\n",
      "Sentiment percentages:\n",
      "  NEU: 55.9%\n",
      "  NEG: 24.0%\n",
      "  POS: 20.1%\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openpyxl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Save the results\u001b[39;00m\n\u001b[32m     22\u001b[39m output_file = \u001b[33m'\u001b[39m\u001b[33mprocessed_monthly_submissions_with_sentiment.xlsx\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mprocessed_monthly_data\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== RESULTS SAVED ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResults saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/pandas/core/generic.py:2436\u001b[39m, in \u001b[36mNDFrame.to_excel\u001b[39m\u001b[34m(self, excel_writer, sheet_name, na_rep, float_format, columns, header, index, index_label, startrow, startcol, engine, merge_cells, inf_rep, freeze_panes, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   2423\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mformats\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexcel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ExcelFormatter\n\u001b[32m   2425\u001b[39m formatter = ExcelFormatter(\n\u001b[32m   2426\u001b[39m     df,\n\u001b[32m   2427\u001b[39m     na_rep=na_rep,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2434\u001b[39m     inf_rep=inf_rep,\n\u001b[32m   2435\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2436\u001b[39m \u001b[43mformatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2437\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexcel_writer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2438\u001b[39m \u001b[43m    \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2439\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2440\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstartcol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2441\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfreeze_panes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2442\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2443\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2444\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2445\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/pandas/io/formats/excel.py:943\u001b[39m, in \u001b[36mExcelFormatter.write\u001b[39m\u001b[34m(self, writer, sheet_name, startrow, startcol, freeze_panes, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m    941\u001b[39m     need_save = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     writer = \u001b[43mExcelWriter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mwriter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    949\u001b[39m     need_save = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    951\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/pandas/io/excel/_openpyxl.py:57\u001b[39m, in \u001b[36mOpenpyxlWriter.__init__\u001b[39m\u001b[34m(self, path, engine, date_format, datetime_format, mode, storage_options, if_sheet_exists, engine_kwargs, **kwargs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m     45\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     46\u001b[39m     path: FilePath | WriteExcelBuffer | ExcelWriter,\n\u001b[32m   (...)\u001b[39m\u001b[32m     55\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     56\u001b[39m     \u001b[38;5;66;03m# Use the openpyxl module as the Excel writer.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mopenpyxl\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mworkbook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Workbook\n\u001b[32m     59\u001b[39m     engine_kwargs = combine_kwargs(engine_kwargs, kwargs)\n\u001b[32m     61\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\n\u001b[32m     62\u001b[39m         path,\n\u001b[32m     63\u001b[39m         mode=mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m         engine_kwargs=engine_kwargs,\n\u001b[32m     67\u001b[39m     )\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'openpyxl'"
     ]
    }
   ],
   "source": [
    "# Process all monthly submissions\n",
    "print(\"Starting to process all monthly submission CSV files...\")\n",
    "processed_monthly_data = process_all_monthly_submissions('individual_submissions_month')\n",
    "\n",
    "# Display summary statistics\n",
    "if len(processed_monthly_data) > 0:\n",
    "    print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
    "    print(f\"Total processed rows: {len(processed_monthly_data)}\")\n",
    "    print(f\"Unique submissions: {processed_monthly_data['submission_title'].nunique()}\")\n",
    "    print(f\"Date range: {processed_monthly_data['created_utc'].min()} to {processed_monthly_data['created_utc'].max()}\")\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    print(f\"\\n=== SENTIMENT DISTRIBUTION ===\")\n",
    "    sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    print(f\"Sentiment percentages:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(processed_monthly_data)) * 100\n",
    "        print(f\"  {sentiment}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Save the results\n",
    "    output_file = 'processed_monthly_submissions_with_sentiment.xlsx'\n",
    "    processed_monthly_data.to_excel(output_file, index=False)\n",
    "    print(f\"\\n=== RESULTS SAVED ===\")\n",
    "    print(f\"Results saved to: {output_file}\")\n",
    "    \n",
    "    # Display first few rows\n",
    "    print(f\"\\n=== SAMPLE DATA ===\")\n",
    "    display_columns = ['submission_title', 'body', 'sentiment_prediction', 'created_utc', 'source_file']\n",
    "    print(processed_monthly_data[display_columns].head())\n",
    "else:\n",
    "    print(\"No data was processed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uoft_senti_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
