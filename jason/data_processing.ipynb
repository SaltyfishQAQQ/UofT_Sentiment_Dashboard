{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c37ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for Reddit API data collection\n",
    "import praw  # For making HTTP requests to Reddit API\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import re\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "import string\n",
    "import os\n",
    "from pysentimiento import create_analyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\n",
    "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb0abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove all special characters from the text except for letters, numbers, and spaces.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def convert_emoji_to_text(text, emoji_wrapper=\"emoji\"):\n",
    "    \"\"\"\n",
    "    Converts emoji in the text to descriptive text.\n",
    "    \n",
    "    This function uses emoji.demojize() to replace any emoji with their\n",
    "    corresponding text (e.g., \"ðŸ˜„\" becomes \":smile:\"). It then wraps the\n",
    "    emoji description using the provided emoji_wrapper.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The original text containing emoji.\n",
    "        emoji_wrapper (str): A string that will be used to wrap the emoji text.\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with emojis converted to wrapped text.\n",
    "    \"\"\"\n",
    "    # Convert emojis to descriptive text (e.g., :smile:)\n",
    "    demojized = emoji.demojize(text)\n",
    "    # Define a wrapper string (e.g., \" emoji \")\n",
    "    wrapper = f\" {emoji_wrapper} \".replace(\"  \", \" \")\n",
    "    # Replace the demojized emoji pattern :emoji_name: with the wrapped emoji_name\n",
    "    # For example, \":smile:\" becomes \" emoji smile emoji \"\n",
    "    result = re.sub(r':([^:\\s]+):', lambda m: wrapper + m.group(1) + wrapper, demojized)\n",
    "    return result\n",
    "\n",
    "def is_valid_word(word):\n",
    "    \"\"\"\n",
    "    Returns True if the word is either:\n",
    "    - Composed solely of punctuation\n",
    "    - Contains at least one English letter (a-z or A-Z)\n",
    "    \"\"\"\n",
    "    # Keep if word is only punctuation\n",
    "    if all(ch in string.punctuation for ch in word):\n",
    "        return True\n",
    "    # Keep if the word contains at least one ASCII letter\n",
    "    if re.search(r'[a-zA-Z]', word):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_non_english(text):\n",
    "    \"\"\"\n",
    "    Splits the text into words and filters out any word that is not an English word,\n",
    "    an emoji, or punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if is_valid_word(word)]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 4. Convert created_utc to readable date format (YYYY-MM-DD)\n",
    "def convert_utc_to_readable(utc_timestamp):\n",
    "    \"\"\"\n",
    "    Convert UTC timestamp to a readable date format (YYYY-MM-DD).\n",
    "    \n",
    "    Args:\n",
    "        utc_timestamp (float): UTC timestamp in seconds since epoch.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted date string or empty string if input is NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Handle NaN values\n",
    "        if pd.isna(utc_timestamp):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to datetime object\n",
    "        dt = datetime.fromtimestamp(utc_timestamp)\n",
    "        \n",
    "        # Format as YYYY-MM-DD HH:MM:SS\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    except (ValueError, TypeError, OSError):\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95970d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(master_df, text_column='body'):\n",
    "    \"\"\"\n",
    "    Iterates through each text in the DataFrame, applies sentiment_analyzer,\n",
    "    and adds two columns: sentiment_prediction and sentiment_prob.\n",
    "    \n",
    "    Args:\n",
    "        master_df (pd.DataFrame): The DataFrame to analyze\n",
    "        text_column (str): The column containing text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added sentiment columns\n",
    "    \"\"\"\n",
    "    senti_pred = []\n",
    "    senti_prob = []\n",
    "    \n",
    "    for index, row in master_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        \n",
    "        if pd.isna(text) or text == \"\":\n",
    "            senti_pred.append(\"NEU\")\n",
    "            senti_prob.append({\"NEU\": 0.34, \"NEG\": 0.33, \"POS\": 0.33})\n",
    "        else:\n",
    "            result = sentiment_analyzer.predict(text)\n",
    "            senti_pred.append(result.output)\n",
    "            senti_prob.append(result.probas)\n",
    "    \n",
    "    master_df['sentiment_prediction'] = senti_pred\n",
    "    master_df['sentiment_prob'] = senti_prob\n",
    "    master_df['sentiment_score'] = master_df['sentiment_prob'].apply(lambda x: x.get('POS', 0) - x.get('NEG', 0))\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "def emotion_analysis(input_df, text_column='body'):\n",
    "    emotion_pred = []\n",
    "    emotion_prob = []\n",
    "    \n",
    "    for index, row in input_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        result = classifier(text)\n",
    "        emotion_pred.append(result[0]['label'])\n",
    "        emotion_prob.append({label['label']: label['score'] for label in result})\n",
    "\n",
    "    input_df['emotion_prediction'] = emotion_pred\n",
    "    input_df['emotion_prob'] = emotion_prob\n",
    "\n",
    "    return input_df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all preprocessing steps to a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # 1. Keep only posts with URLs starting with UofT comments\n",
    "    processed_df = processed_df[processed_df['url'].str.startswith('https://www.reddit.com/r/UofT/comments/')].reset_index(drop=True).copy()\n",
    "    \n",
    "    # 2. Clean body text: lowercase, strip, remove special chars except spaces\n",
    "    processed_df['body'] = processed_df['body'].str.lower()\n",
    "    processed_df['body'] = processed_df['body'].str.strip()\n",
    "    \n",
    "    # 3. Remove special characters except spaces\n",
    "    processed_df['body'] = processed_df['body'].apply(clean_text)\n",
    "    \n",
    "    # 4. Converts emoji in the text to descriptive text\n",
    "    processed_df['body'] = processed_df['body'].apply(convert_emoji_to_text)\n",
    "    \n",
    "    # 5. Filter out non-English words\n",
    "    processed_df['body'] = processed_df['body'].apply(filter_non_english)\n",
    "    \n",
    "    # 6. Convert created_utc to readable date format\n",
    "    processed_df['created_utc'] = processed_df['created_utc'].apply(convert_utc_to_readable)\n",
    "    \n",
    "    # 7. Preprocess the body column using pysentimiento (optional - currently commented out)\n",
    "    # processed_df['body'] = processed_df['body'].apply(preprocess_tweet)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def process_all_monthly_submissions(folder_path='individual_submissions_month', senti=True, emo=True):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the monthly submissions folder.\n",
    "    Apply preprocessing and sentiment analysis to each file.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all processed submissions\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    csv_files.sort()  # Sort to process in order\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    all_processed_data = []\n",
    "    \n",
    "    for i, filename in enumerate(csv_files):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file {i+1}/{len(csv_files)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  - Loaded {len(df)} rows\")\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            processed_df = preprocess_dataframe(df)\n",
    "            print(f\"  - After filtering: {len(processed_df)} rows\")\n",
    "            \n",
    "            # Apply sentiment analysis\n",
    "            if len(processed_df) > 0:\n",
    "                if senti:\n",
    "                    processed_df = sentiment_analysis(processed_df, text_column='body')\n",
    "                    print(f\"  - Sentiment analysis completed\")\n",
    "\n",
    "                if emo:\n",
    "                    processed_df = emotion_analysis(processed_df, text_column='body')\n",
    "                    print(f\"  - Emotion analysis completed\")\n",
    "                \n",
    "                # Add source file information\n",
    "                processed_df['source_file'] = filename\n",
    "                \n",
    "                all_processed_data.append(processed_df)\n",
    "            else:\n",
    "                print(f\"  - No data remaining after filtering\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all processed data\n",
    "    if all_processed_data:\n",
    "        combined_df = pd.concat(all_processed_data, ignore_index=True)\n",
    "        print(f\"\\nProcessing complete! Combined dataset has {len(combined_df)} rows\")\n",
    "        return all_processed_data, combined_df\n",
    "    else:\n",
    "        print(\"\\nNo data was successfully processed\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# processed_monthly_data = process_all_monthly_submissions('individual_submissions_month')\n",
    "# processed_monthly_data.to_excel('processed_monthly_submissions_with_sentiment.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e7de78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process all monthly submission CSV files...\n",
      "Found 99 CSV files to process\n",
      "Processing file 1/99: submission_0.csv\n",
      "  - Loaded 178 rows\n",
      "  - After filtering: 177 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 2/99: submission_1.csv\n",
      "  - Loaded 65 rows\n",
      "  - After filtering: 64 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 3/99: submission_10.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 4/99: submission_11.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 5/99: submission_12.csv\n",
      "  - Loaded 40 rows\n",
      "  - After filtering: 39 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 6/99: submission_13.csv\n",
      "  - Loaded 97 rows\n",
      "  - After filtering: 97 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 7/99: submission_14.csv\n",
      "  - Loaded 73 rows\n",
      "  - After filtering: 72 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 8/99: submission_15.csv\n",
      "  - Loaded 20 rows\n",
      "  - After filtering: 20 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 9/99: submission_16.csv\n",
      "  - Loaded 35 rows\n",
      "  - After filtering: 35 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 10/99: submission_17.csv\n",
      "  - Loaded 31 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 11/99: submission_18.csv\n",
      "  - Loaded 36 rows\n",
      "  - After filtering: 36 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 12/99: submission_19.csv\n",
      "  - Loaded 89 rows\n",
      "  - After filtering: 89 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 13/99: submission_2.csv\n",
      "  - Loaded 35 rows\n",
      "  - After filtering: 35 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 14/99: submission_20.csv\n",
      "  - Loaded 47 rows\n",
      "  - After filtering: 46 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 15/99: submission_21.csv\n",
      "  - Loaded 61 rows\n",
      "  - After filtering: 60 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 16/99: submission_22.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 17/99: submission_23.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 18/99: submission_24.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 19/99: submission_25.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 20/99: submission_26.csv\n",
      "  - Loaded 22 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 21/99: submission_27.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 22/99: submission_28.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 23/99: submission_29.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 24/99: submission_3.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 25/99: submission_30.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 26/99: submission_31.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 27/99: submission_32.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 28/99: submission_33.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 20 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 29/99: submission_34.csv\n",
      "  - Loaded 38 rows\n",
      "  - After filtering: 38 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 30/99: submission_35.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 31/99: submission_36.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 32/99: submission_37.csv\n",
      "  - Loaded 88 rows\n",
      "  - After filtering: 87 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 33/99: submission_38.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 34/99: submission_39.csv\n",
      "  - Loaded 20 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 35/99: submission_4.csv\n",
      "  - Loaded 83 rows\n",
      "  - After filtering: 83 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 36/99: submission_40.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 37/99: submission_41.csv\n",
      "  - Loaded 45 rows\n",
      "  - After filtering: 45 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 38/99: submission_42.csv\n",
      "  - Loaded 29 rows\n",
      "  - After filtering: 29 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 39/99: submission_43.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 40/99: submission_44.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 41/99: submission_45.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 42/99: submission_46.csv\n",
      "  - Loaded 53 rows\n",
      "  - After filtering: 53 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 43/99: submission_47.csv\n",
      "  - Loaded 28 rows\n",
      "  - After filtering: 28 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 44/99: submission_48.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 45/99: submission_49.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 46/99: submission_5.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 47/99: submission_50.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 48/99: submission_51.csv\n",
      "  - Loaded 73 rows\n",
      "  - After filtering: 72 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 49/99: submission_52.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 50/99: submission_53.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 51/99: submission_54.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 52/99: submission_55.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 53/99: submission_56.csv\n",
      "  - Loaded 28 rows\n",
      "  - After filtering: 28 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 54/99: submission_57.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 55/99: submission_58.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 56/99: submission_59.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 57/99: submission_6.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 58/99: submission_60.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 59/99: submission_61.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 60/99: submission_62.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 61/99: submission_63.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 62/99: submission_64.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 63/99: submission_65.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 64/99: submission_66.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 65/99: submission_67.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 66/99: submission_68.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 67/99: submission_69.csv\n",
      "  - Loaded 31 rows\n",
      "  - After filtering: 30 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 68/99: submission_7.csv\n",
      "  - Loaded 34 rows\n",
      "  - After filtering: 34 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 69/99: submission_70.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 70/99: submission_71.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 71/99: submission_72.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 72/99: submission_73.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 73/99: submission_74.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 74/99: submission_75.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 75/99: submission_76.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 76/99: submission_77.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 77/99: submission_78.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 1 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 78/99: submission_79.csv\n",
      "  - Loaded 20 rows\n",
      "  - After filtering: 20 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 79/99: submission_8.csv\n",
      "  - Loaded 78 rows\n",
      "  - After filtering: 78 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 80/99: submission_80.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 81/99: submission_81.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 82/99: submission_82.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 83/99: submission_83.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 84/99: submission_84.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 3 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 85/99: submission_85.csv\n",
      "  - Loaded 34 rows\n",
      "  - After filtering: 34 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 86/99: submission_86.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 87/99: submission_87.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 88/99: submission_88.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 89/99: submission_89.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 90/99: submission_9.csv\n",
      "  - Loaded 43 rows\n",
      "  - After filtering: 43 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 91/99: submission_90.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 92/99: submission_91.csv\n",
      "  - Loaded 31 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 93/99: submission_92.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 94/99: submission_93.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 95/99: submission_94.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 96/99: submission_95.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 97/99: submission_96.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 98/99: submission_97.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 1 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 99/99: submission_98.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "\n",
      "Processing complete! Combined dataset has 2561 rows\n",
      "\n",
      "=== PROCESSING SUMMARY ===\n",
      "Total processed rows: 2561\n",
      "Unique submissions: 99\n",
      "Date range: 2025-08-21 17:40:16 to 2025-09-20 10:30:56\n",
      "\n",
      "=== SENTIMENT DISTRIBUTION ===\n",
      "sentiment_prediction\n",
      "NEU    1337\n",
      "NEG     753\n",
      "POS     471\n",
      "Name: count, dtype: int64\n",
      "Sentiment percentages:\n",
      "  NEU: 52.2%\n",
      "  NEG: 29.4%\n",
      "  POS: 18.4%\n"
     ]
    }
   ],
   "source": [
    "# Process all monthly submissions\n",
    "print(\"Starting to process all monthly submission CSV files...\")\n",
    "today = datetime.now().strftime('%Y-%m')\n",
    "folder_path = f'month_top100/{today}'\n",
    "all_processed_data, processed_monthly_data = process_all_monthly_submissions(folder_path, senti=True, emo=False)\n",
    "\n",
    "# Display summary statistics\n",
    "if len(processed_monthly_data) > 0:\n",
    "    print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
    "    print(f\"Total processed rows: {len(processed_monthly_data)}\")\n",
    "    print(f\"Unique submissions: {processed_monthly_data['submission_title'].nunique()}\")\n",
    "    print(f\"Date range: {processed_monthly_data['created_utc'].min()} to {processed_monthly_data['created_utc'].max()}\")\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    print(f\"\\n=== SENTIMENT DISTRIBUTION ===\")\n",
    "    sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    print(f\"Sentiment percentages:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(processed_monthly_data)) * 100\n",
    "        print(f\"  {sentiment}: {percentage:.1f}%\")\n",
    "else:\n",
    "    print(\"No data was processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6381a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now().strftime('%Y%m%d')\n",
    "file_name = f'top_100_reddits_{today}.csv'\n",
    "processed_monthly_data.to_csv(file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e941a7e",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'emotion_prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'emotion_prediction'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m sentiment_counts = processed_monthly_data[\u001b[33m'\u001b[39m\u001b[33msentiment_prediction\u001b[39m\u001b[33m'\u001b[39m].value_counts()\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m emotion_counts = \u001b[43mprocessed_monthly_data\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43memotion_prediction\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m.value_counts()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/pandas/core/frame.py:4107\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4105\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4107\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4108\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4109\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/pandas/core/indexes/base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'emotion_prediction'"
     ]
    }
   ],
   "source": [
    "sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n",
    "emotion_counts = processed_monthly_data['emotion_prediction'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdea175",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f865dc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion_prediction\n",
       "neutral     1171\n",
       "joy          159\n",
       "sadness      111\n",
       "anger         81\n",
       "surprise      36\n",
       "fear          20\n",
       "disgust       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db87fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uoft_senti_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
