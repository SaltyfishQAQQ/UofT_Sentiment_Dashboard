{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c37ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/transformers/pipelines/text_classification.py:111: UserWarning: `return_all_scores` is now deprecated,  if want a similar functionality use `top_k=None` instead of `return_all_scores=True` or `top_k=1` instead of `return_all_scores=False`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for Reddit API data collection\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import re\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "import string\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pysentimiento import create_analyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"j-hartmann/emotion-english-distilroberta-base\", return_all_scores=True)\n",
    "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb0abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove all special characters from the text except for letters, numbers, and spaces.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def convert_emoji_to_text(text, emoji_wrapper=\"emoji\"):\n",
    "    \"\"\"\n",
    "    Converts emoji in the text to descriptive text.\n",
    "    \n",
    "    This function uses emoji.demojize() to replace any emoji with their\n",
    "    corresponding text (e.g., \"ðŸ˜„\" becomes \":smile:\"). It then wraps the\n",
    "    emoji description using the provided emoji_wrapper.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The original text containing emoji.\n",
    "        emoji_wrapper (str): A string that will be used to wrap the emoji text.\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with emojis converted to wrapped text.\n",
    "    \"\"\"\n",
    "    # Convert emojis to descriptive text (e.g., :smile:)\n",
    "    demojized = emoji.demojize(text)\n",
    "    # Define a wrapper string (e.g., \" emoji \")\n",
    "    wrapper = f\" {emoji_wrapper} \".replace(\"  \", \" \")\n",
    "    # Replace the demojized emoji pattern :emoji_name: with the wrapped emoji_name\n",
    "    # For example, \":smile:\" becomes \" emoji smile emoji \"\n",
    "    result = re.sub(r':([^:\\s]+):', lambda m: wrapper + m.group(1) + wrapper, demojized)\n",
    "    return result\n",
    "\n",
    "def is_valid_word(word):\n",
    "    \"\"\"\n",
    "    Returns True if the word is either:\n",
    "    - Composed solely of punctuation\n",
    "    - Contains at least one English letter (a-z or A-Z)\n",
    "    \"\"\"\n",
    "    # Keep if word is only punctuation\n",
    "    if all(ch in string.punctuation for ch in word):\n",
    "        return True\n",
    "    # Keep if the word contains at least one ASCII letter\n",
    "    if re.search(r'[a-zA-Z]', word):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_non_english(text):\n",
    "    \"\"\"\n",
    "    Splits the text into words and filters out any word that is not an English word,\n",
    "    an emoji, or punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if is_valid_word(word)]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 4. Convert created_utc to readable date format (YYYY-MM-DD)\n",
    "def convert_utc_to_readable(utc_timestamp):\n",
    "    \"\"\"\n",
    "    Convert UTC timestamp to a readable date format (YYYY-MM-DD).\n",
    "    \n",
    "    Args:\n",
    "        utc_timestamp (float): UTC timestamp in seconds since epoch.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted date string or empty string if input is NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Handle NaN values\n",
    "        if pd.isna(utc_timestamp):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to datetime object\n",
    "        dt = datetime.fromtimestamp(utc_timestamp)\n",
    "        \n",
    "        # Format as YYYY-MM-DD HH:MM:SS\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    except (ValueError, TypeError, OSError):\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95970d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(master_df, text_column='body'):\n",
    "    \"\"\"\n",
    "    Iterates through each text in the DataFrame, applies sentiment_analyzer,\n",
    "    and adds two columns: sentiment_prediction and sentiment_prob.\n",
    "    \n",
    "    Args:\n",
    "        master_df (pd.DataFrame): The DataFrame to analyze\n",
    "        text_column (str): The column containing text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added sentiment columns\n",
    "    \"\"\"\n",
    "    senti_pred = []\n",
    "    senti_prob = []\n",
    "    \n",
    "    for index, row in master_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        \n",
    "        if pd.isna(text) or text == \"\":\n",
    "            senti_pred.append(\"NEU\")\n",
    "            senti_prob.append({\"NEU\": 0.34, \"NEG\": 0.33, \"POS\": 0.33})\n",
    "        else:\n",
    "            result = sentiment_analyzer.predict(text)\n",
    "            senti_pred.append(result.output)\n",
    "            senti_prob.append(result.probas)\n",
    "    \n",
    "    master_df['sentiment_prediction'] = senti_pred\n",
    "    master_df['sentiment_prob'] = senti_prob\n",
    "    master_df['sentiment_score'] = master_df['sentiment_prob'].apply(lambda x: x.get('POS', 0) - x.get('NEG', 0))\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "def emotion_analysis(input_df, text_column='body'):\n",
    "    emotion_pred = []\n",
    "    emotion_prob = []\n",
    "    \n",
    "    for index, row in input_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        result = classifier(text)\n",
    "        \n",
    "\t\t# Sort results by score in descending order\n",
    "        sorted_results = sorted(result[0], key=lambda x: x['score'], reverse=True)\n",
    "        \n",
    "        # Get top emotion and its confidence\n",
    "        top_emotion = sorted_results[0]\n",
    "        top_confidence = top_emotion['score']\n",
    "        \n",
    "        if top_confidence >= 0.7:\n",
    "            # Use only top emotion if confidence >= 70%\n",
    "            emotion_pred.append(top_emotion['label'])\n",
    "            emotion_prob.append({top_emotion['label']: top_emotion['score']})\n",
    "        else:\n",
    "            # Use top 2 emotions if confidence < 70%\n",
    "            top_2_emotions = sorted_results[:2]\n",
    "            labels = [emotion['label'] for emotion in top_2_emotions]\n",
    "            emotion_pred.append(labels)  # Store as list of 2 emotions\n",
    "            emotion_prob.append({emotion['label']: emotion['score'] for emotion in top_2_emotions})\n",
    "\n",
    "    input_df['emotion_prediction'] = emotion_pred\n",
    "    input_df['emotion_prob'] = emotion_prob\n",
    "\n",
    "    return input_df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all preprocessing steps to a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # 1. Keep only posts with URLs starting with UofT comments\n",
    "    processed_df = processed_df[processed_df['url'].str.startswith('https://www.reddit.com/r/UofT/comments/')].reset_index(drop=True).copy()\n",
    "    \n",
    "    # 2. Clean body text: lowercase, strip, remove special chars except spaces\n",
    "    processed_df['body'] = processed_df['body'].str.lower()\n",
    "    processed_df['body'] = processed_df['body'].str.strip()\n",
    "    \n",
    "    # 3. Remove special characters except spaces\n",
    "    processed_df['body'] = processed_df['body'].apply(clean_text)\n",
    "    \n",
    "    # 4. Converts emoji in the text to descriptive text\n",
    "    processed_df['body'] = processed_df['body'].apply(convert_emoji_to_text)\n",
    "    \n",
    "    # 5. Filter out non-English words\n",
    "    processed_df['body'] = processed_df['body'].apply(filter_non_english)\n",
    "    \n",
    "    # 6. Convert created_utc to readable date format\n",
    "    processed_df['created_utc'] = processed_df['created_utc'].apply(convert_utc_to_readable)\n",
    "    \n",
    "    # 7. Preprocess the body column using pysentimiento (optional - currently commented out)\n",
    "    # processed_df['body'] = processed_df['body'].apply(preprocess_tweet)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def process_all_monthly_submissions(folder_path, senti=True, emo=True):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the monthly submissions folder.\n",
    "    Apply preprocessing and sentiment analysis to each file.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all processed submissions\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    csv_files.sort()  # Sort to process in order\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    all_processed_data = []\n",
    "    \n",
    "    for i, filename in enumerate(csv_files):\n",
    "        if filename.startswith('top_100_reddits_'):\n",
    "            continue  # Skip the combined file if it exists\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file {i+1}/{len(csv_files)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  - Loaded {len(df)} rows\")\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            processed_df = preprocess_dataframe(df)\n",
    "            print(f\"  - After filtering: {len(processed_df)} rows\")\n",
    "            \n",
    "            # Apply sentiment analysis\n",
    "            if len(processed_df) > 0:\n",
    "                if senti:\n",
    "                    processed_df = sentiment_analysis(processed_df, text_column='body')\n",
    "                    print(f\"  - Sentiment analysis completed\")\n",
    "\n",
    "                if emo:\n",
    "                    processed_df = emotion_analysis(processed_df, text_column='body')\n",
    "                    print(f\"  - Emotion analysis completed\")\n",
    "                \n",
    "                # Add source file information\n",
    "                processed_df['source_file'] = filename\n",
    "                \n",
    "                all_processed_data.append(processed_df)\n",
    "            else:\n",
    "                print(f\"  - No data remaining after filtering\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all processed data\n",
    "    if all_processed_data:\n",
    "        combined_df = pd.concat(all_processed_data, ignore_index=True)\n",
    "        print(f\"\\nProcessing complete! Combined dataset has {len(combined_df)} rows\")\n",
    "        return all_processed_data, combined_df\n",
    "    else:\n",
    "        print(\"\\nNo data was successfully processed\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# processed_monthly_data = process_all_monthly_submissions('individual_submissions_month')\n",
    "# processed_monthly_data.to_excel('processed_monthly_submissions_with_sentiment.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e7de78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process all monthly submission CSV files...\n",
      "Found 98 CSV files to process\n",
      "Processing file 1/98: submission_0.csv\n",
      "  - Loaded 90 rows\n",
      "  - After filtering: 89 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 2/98: submission_1.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 3/98: submission_10.csv\n",
      "  - Loaded 61 rows\n",
      "  - After filtering: 60 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 4/98: submission_11.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_11.csv: The expanded size of the tensor (553) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 553].  Tensor sizes: [1, 514]\n",
      "Processing file 5/98: submission_12.csv\n",
      "  - Loaded 42 rows\n",
      "  - After filtering: 42 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 6/98: submission_13.csv\n",
      "  - Loaded 40 rows\n",
      "  - After filtering: 39 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 7/98: submission_14.csv\n",
      "  - Loaded 37 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 8/98: submission_15.csv\n",
      "  - Loaded 44 rows\n",
      "  - After filtering: 44 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 9/98: submission_16.csv\n",
      "  - Loaded 96 rows\n",
      "  - After filtering: 96 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 10/98: submission_17.csv\n",
      "  - Loaded 22 rows\n",
      "  - After filtering: 22 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 11/98: submission_18.csv\n",
      "  - Loaded 119 rows\n",
      "  - After filtering: 119 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 12/98: submission_19.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 13/98: submission_2.csv\n",
      "  - Loaded 22 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 14/98: submission_20.csv\n",
      "  - Loaded 65 rows\n",
      "  - After filtering: 64 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 15/98: submission_21.csv\n",
      "  - Loaded 87 rows\n",
      "  - After filtering: 87 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 16/98: submission_22.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 17/98: submission_23.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 18/98: submission_24.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 19/98: submission_25.csv\n",
      "  - Loaded 55 rows\n",
      "  - After filtering: 55 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 20/98: submission_26.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 21/98: submission_27.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 22/98: submission_28.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 23/98: submission_29.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 24/98: submission_3.csv\n",
      "  - Loaded 43 rows\n",
      "  - After filtering: 43 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 25/98: submission_30.csv\n",
      "  - Loaded 31 rows\n",
      "  - After filtering: 30 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 26/98: submission_31.csv\n",
      "  - Loaded 147 rows\n",
      "  - After filtering: 147 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 27/98: submission_32.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 28/98: submission_33.csv\n",
      "  - Loaded 65 rows\n",
      "  - After filtering: 65 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 29/98: submission_34.csv\n",
      "  - Loaded 100 rows\n",
      "  - After filtering: 100 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 30/98: submission_35.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 31/98: submission_36.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 32/98: submission_37.csv\n",
      "  - Loaded 20 rows\n",
      "  - After filtering: 20 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 33/98: submission_38.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 34/98: submission_39.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 35/98: submission_4.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 36/98: submission_40.csv\n",
      "  - Loaded 44 rows\n",
      "  - After filtering: 44 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 37/98: submission_41.csv\n",
      "  - Loaded 37 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 38/98: submission_42.csv\n",
      "  - Loaded 36 rows\n",
      "  - After filtering: 36 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 39/98: submission_43.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 40/98: submission_44.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 41/98: submission_45.csv\n",
      "  - Loaded 29 rows\n",
      "  - After filtering: 29 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 42/98: submission_46.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 43/98: submission_47.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 44/98: submission_48.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 45/98: submission_49.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 46/98: submission_5.csv\n",
      "  - Loaded 20 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 47/98: submission_50.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 48/98: submission_51.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 49/98: submission_52.csv\n",
      "  - Loaded 47 rows\n",
      "  - After filtering: 47 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 50/98: submission_53.csv\n",
      "  - Loaded 29 rows\n",
      "  - After filtering: 29 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 51/98: submission_54.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 52/98: submission_55.csv\n",
      "  - Loaded 37 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 53/98: submission_56.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 54/98: submission_57.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 55/98: submission_58.csv\n",
      "  - Loaded 29 rows\n",
      "  - After filtering: 29 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 56/98: submission_59.csv\n",
      "  - Loaded 42 rows\n",
      "  - After filtering: 42 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 57/98: submission_6.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 58/98: submission_60.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 59/98: submission_61.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 60/98: submission_62.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 61/98: submission_63.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 62/98: submission_64.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 63/98: submission_65.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 64/98: submission_66.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 65/98: submission_67.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 66/98: submission_68.csv\n",
      "  - Loaded 60 rows\n",
      "  - After filtering: 60 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 67/98: submission_69.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 68/98: submission_7.csv\n",
      "  - Loaded 44 rows\n",
      "  - After filtering: 44 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_7.csv: The expanded size of the tensor (643) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 643].  Tensor sizes: [1, 514]\n",
      "Processing file 69/98: submission_70.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 70/98: submission_71.csv\n",
      "  - Loaded 28 rows\n",
      "  - After filtering: 28 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 71/98: submission_72.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 72/98: submission_73.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 73/98: submission_74.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 74/98: submission_75.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 75/98: submission_76.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 76/98: submission_77.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 27 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 77/98: submission_78.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 78/98: submission_79.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 79/98: submission_8.csv\n",
      "  - Loaded 78 rows\n",
      "  - After filtering: 78 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 80/98: submission_80.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 81/98: submission_81.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 3 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_81.csv: The expanded size of the tensor (550) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 550].  Tensor sizes: [1, 514]\n",
      "Processing file 82/98: submission_82.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 83/98: submission_83.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 84/98: submission_84.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 85/98: submission_85.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 86/98: submission_86.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 1 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 87/98: submission_87.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 88/98: submission_88.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 89/98: submission_89.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 90/98: submission_9.csv\n",
      "  - Loaded 34 rows\n",
      "  - After filtering: 34 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 91/98: submission_90.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 92/98: submission_91.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 93/98: submission_92.csv\n",
      "  - Loaded 25 rows\n",
      "  - After filtering: 25 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 94/98: submission_93.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 95/98: submission_94.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 96/98: submission_95.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 97/98: submission_96.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "\n",
      "Processing complete! Combined dataset has 2542 rows\n",
      "\n",
      "=== PROCESSING SUMMARY ===\n",
      "Total processed rows: 2542\n",
      "Unique submissions: 94\n",
      "Date range: 2025-09-10 16:50:18 to 2025-10-10 12:58:04\n",
      "\n",
      "=== SENTIMENT DISTRIBUTION ===\n",
      "sentiment_prediction\n",
      "NEU    1338\n",
      "NEG     775\n",
      "POS     429\n",
      "Name: count, dtype: int64\n",
      "Sentiment percentages:\n",
      "  NEU: 52.6%\n",
      "  NEG: 30.5%\n",
      "  POS: 16.9%\n"
     ]
    }
   ],
   "source": [
    "# Process all monthly submissions\n",
    "print(\"Starting to process all monthly submission CSV files...\")\n",
    "today = datetime.now().strftime('%Y%m')\n",
    "folder_path = f'monthly_top100/{today}'\n",
    "all_processed_data, processed_monthly_data = process_all_monthly_submissions(folder_path, senti=True, emo=True)\n",
    "\n",
    "# Display summary statistics\n",
    "if len(processed_monthly_data) > 0:\n",
    "    print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
    "    print(f\"Total processed rows: {len(processed_monthly_data)}\")\n",
    "    print(f\"Unique submissions: {processed_monthly_data['submission_title'].nunique()}\")\n",
    "    print(f\"Date range: {processed_monthly_data['created_utc'].min()} to {processed_monthly_data['created_utc'].max()}\")\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    print(f\"\\n=== SENTIMENT DISTRIBUTION ===\")\n",
    "    sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    print(f\"Sentiment percentages:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(processed_monthly_data)) * 100\n",
    "        print(f\"  {sentiment}: {percentage:.1f}%\")\n",
    "else:\n",
    "    print(\"No data was processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6381a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now().strftime('%Y%m')\n",
    "file_name = Path(f'top_100_reddits_{today}.csv')\n",
    "processed_monthly_data.to_csv(folder_path / file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e941a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a730e8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment_prediction</th>\n",
       "      <th>sentiment_prob</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>emotion_prediction</th>\n",
       "      <th>emotion_prob</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>buckbuck5645</td>\n",
       "      <td>nes2v1g</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 17:35:18</td>\n",
       "      <td>this might be one of the dumbest questions ive...</td>\n",
       "      <td>comment</td>\n",
       "      <td>274</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.978022575378418, 'NEU': 0.0189272090...</td>\n",
       "      <td>-0.974972</td>\n",
       "      <td>[disgust, sadness]</td>\n",
       "      <td>{'disgust': 0.36396047472953796, 'sadness': 0....</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>SNG_Blitzy</td>\n",
       "      <td>nesxtjv</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 20:29:13</td>\n",
       "      <td>its essentially the same thing but its crucial...</td>\n",
       "      <td>comment</td>\n",
       "      <td>128</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.028543880209326744, 'NEU': 0.9490432...</td>\n",
       "      <td>-0.006131</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.8884442448616028}</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>BromineFromine</td>\n",
       "      <td>nes8way</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 18:07:21</td>\n",
       "      <td>when you put a vengeful pedant in charge of yo...</td>\n",
       "      <td>comment</td>\n",
       "      <td>61</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.9388996958732605, 'NEU': 0.057808689...</td>\n",
       "      <td>-0.935608</td>\n",
       "      <td>anger</td>\n",
       "      <td>{'anger': 0.9504784345626831}</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>onlyonequickquestion</td>\n",
       "      <td>neskxvd</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 19:15:21</td>\n",
       "      <td>well maybe in a technical or legal sense there...</td>\n",
       "      <td>comment</td>\n",
       "      <td>40</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.801746666431427, 'NEU': 0.1943267583...</td>\n",
       "      <td>-0.797820</td>\n",
       "      <td>[neutral, anger]</td>\n",
       "      <td>{'neutral': 0.5479668378829956, 'anger': 0.251...</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>BackgroundBench530</td>\n",
       "      <td>netfy6p</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 22:14:05</td>\n",
       "      <td>im a criminologist even the correct answer is ...</td>\n",
       "      <td>comment</td>\n",
       "      <td>20</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.33407503366470337, 'NEU': 0.63717758...</td>\n",
       "      <td>-0.305328</td>\n",
       "      <td>[anger, fear]</td>\n",
       "      <td>{'anger': 0.4802381694316864, 'fear': 0.261093...</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2537</th>\n",
       "      <td>So whatâ€™s with the people protesting abortions...</td>\n",
       "      <td>OdditiesInOntario</td>\n",
       "      <td>nfavgkm</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1nlnfuw...</td>\n",
       "      <td>2025-09-20 15:56:15</td>\n",
       "      <td>which is wholly and totally irrelevant here be...</td>\n",
       "      <td>comment</td>\n",
       "      <td>1</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.3910699188709259, 'NEU': 0.600081980...</td>\n",
       "      <td>-0.382222</td>\n",
       "      <td>anger</td>\n",
       "      <td>{'anger': 0.9745969176292419}</td>\n",
       "      <td>submission_96.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2538</th>\n",
       "      <td>So whatâ€™s with the people protesting abortions...</td>\n",
       "      <td>torontopeter</td>\n",
       "      <td>nf8hx2h</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1nlnfuw...</td>\n",
       "      <td>2025-09-20 08:26:53</td>\n",
       "      <td>not disagreeing with that these despicable peo...</td>\n",
       "      <td>comment</td>\n",
       "      <td>-3</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.9640244245529175, 'NEU': 0.033359553...</td>\n",
       "      <td>-0.961408</td>\n",
       "      <td>disgust</td>\n",
       "      <td>{'disgust': 0.9778687357902527}</td>\n",
       "      <td>submission_96.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2539</th>\n",
       "      <td>So whatâ€™s with the people protesting abortions...</td>\n",
       "      <td>ThatGenericName2</td>\n",
       "      <td>nf8j5uv</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1nlnfuw...</td>\n",
       "      <td>2025-09-20 08:35:19</td>\n",
       "      <td>ok what were you saying with your original com...</td>\n",
       "      <td>comment</td>\n",
       "      <td>5</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.01563001424074173, 'NEU': 0.96736568...</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>surprise</td>\n",
       "      <td>{'surprise': 0.8578999042510986}</td>\n",
       "      <td>submission_96.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2540</th>\n",
       "      <td>So whatâ€™s with the people protesting abortions...</td>\n",
       "      <td>torontopeter</td>\n",
       "      <td>nf8jk8d</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1nlnfuw...</td>\n",
       "      <td>2025-09-20 08:37:55</td>\n",
       "      <td>correcting your mistake re the charter</td>\n",
       "      <td>comment</td>\n",
       "      <td>-1</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.32758137583732605, 'NEU': 0.66043359...</td>\n",
       "      <td>-0.315596</td>\n",
       "      <td>[sadness, anger]</td>\n",
       "      <td>{'sadness': 0.4839746654033661, 'anger': 0.256...</td>\n",
       "      <td>submission_96.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2541</th>\n",
       "      <td>So whatâ€™s with the people protesting abortions...</td>\n",
       "      <td>ThatGenericName2</td>\n",
       "      <td>nf8k0gv</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1nlnfuw...</td>\n",
       "      <td>2025-09-20 08:40:52</td>\n",
       "      <td>at what point did i say it applied to individu...</td>\n",
       "      <td>comment</td>\n",
       "      <td>6</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.013485678471624851, 'NEU': 0.9515011...</td>\n",
       "      <td>0.021528</td>\n",
       "      <td>[neutral, joy]</td>\n",
       "      <td>{'neutral': 0.6780093312263489, 'joy': 0.12183...</td>\n",
       "      <td>submission_96.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2542 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       submission_title                author  \\\n",
       "0     these types of questions are actually so dumb ...          buckbuck5645   \n",
       "1     these types of questions are actually so dumb ...            SNG_Blitzy   \n",
       "2     these types of questions are actually so dumb ...        BromineFromine   \n",
       "3     these types of questions are actually so dumb ...  onlyonequickquestion   \n",
       "4     these types of questions are actually so dumb ...    BackgroundBench530   \n",
       "...                                                 ...                   ...   \n",
       "2537  So whatâ€™s with the people protesting abortions...     OdditiesInOntario   \n",
       "2538  So whatâ€™s with the people protesting abortions...          torontopeter   \n",
       "2539  So whatâ€™s with the people protesting abortions...      ThatGenericName2   \n",
       "2540  So whatâ€™s with the people protesting abortions...          torontopeter   \n",
       "2541  So whatâ€™s with the people protesting abortions...      ThatGenericName2   \n",
       "\n",
       "           id                                                url  \\\n",
       "0     nes2v1g  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "1     nesxtjv  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "2     nes8way  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "3     neskxvd  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "4     netfy6p  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "...       ...                                                ...   \n",
       "2537  nfavgkm  https://www.reddit.com/r/UofT/comments/1nlnfuw...   \n",
       "2538  nf8hx2h  https://www.reddit.com/r/UofT/comments/1nlnfuw...   \n",
       "2539  nf8j5uv  https://www.reddit.com/r/UofT/comments/1nlnfuw...   \n",
       "2540  nf8jk8d  https://www.reddit.com/r/UofT/comments/1nlnfuw...   \n",
       "2541  nf8k0gv  https://www.reddit.com/r/UofT/comments/1nlnfuw...   \n",
       "\n",
       "              created_utc                                               body  \\\n",
       "0     2025-09-17 17:35:18  this might be one of the dumbest questions ive...   \n",
       "1     2025-09-17 20:29:13  its essentially the same thing but its crucial...   \n",
       "2     2025-09-17 18:07:21  when you put a vengeful pedant in charge of yo...   \n",
       "3     2025-09-17 19:15:21  well maybe in a technical or legal sense there...   \n",
       "4     2025-09-17 22:14:05  im a criminologist even the correct answer is ...   \n",
       "...                   ...                                                ...   \n",
       "2537  2025-09-20 15:56:15  which is wholly and totally irrelevant here be...   \n",
       "2538  2025-09-20 08:26:53  not disagreeing with that these despicable peo...   \n",
       "2539  2025-09-20 08:35:19  ok what were you saying with your original com...   \n",
       "2540  2025-09-20 08:37:55             correcting your mistake re the charter   \n",
       "2541  2025-09-20 08:40:52  at what point did i say it applied to individu...   \n",
       "\n",
       "         type  score sentiment_prediction  \\\n",
       "0     comment    274                  NEG   \n",
       "1     comment    128                  NEU   \n",
       "2     comment     61                  NEG   \n",
       "3     comment     40                  NEG   \n",
       "4     comment     20                  NEU   \n",
       "...       ...    ...                  ...   \n",
       "2537  comment      1                  NEU   \n",
       "2538  comment     -3                  NEG   \n",
       "2539  comment      5                  NEU   \n",
       "2540  comment     -1                  NEU   \n",
       "2541  comment      6                  NEU   \n",
       "\n",
       "                                         sentiment_prob  sentiment_score  \\\n",
       "0     {'NEG': 0.978022575378418, 'NEU': 0.0189272090...        -0.974972   \n",
       "1     {'NEG': 0.028543880209326744, 'NEU': 0.9490432...        -0.006131   \n",
       "2     {'NEG': 0.9388996958732605, 'NEU': 0.057808689...        -0.935608   \n",
       "3     {'NEG': 0.801746666431427, 'NEU': 0.1943267583...        -0.797820   \n",
       "4     {'NEG': 0.33407503366470337, 'NEU': 0.63717758...        -0.305328   \n",
       "...                                                 ...              ...   \n",
       "2537  {'NEG': 0.3910699188709259, 'NEU': 0.600081980...        -0.382222   \n",
       "2538  {'NEG': 0.9640244245529175, 'NEU': 0.033359553...        -0.961408   \n",
       "2539  {'NEG': 0.01563001424074173, 'NEU': 0.96736568...         0.001374   \n",
       "2540  {'NEG': 0.32758137583732605, 'NEU': 0.66043359...        -0.315596   \n",
       "2541  {'NEG': 0.013485678471624851, 'NEU': 0.9515011...         0.021528   \n",
       "\n",
       "      emotion_prediction                                       emotion_prob  \\\n",
       "0     [disgust, sadness]  {'disgust': 0.36396047472953796, 'sadness': 0....   \n",
       "1                neutral                    {'neutral': 0.8884442448616028}   \n",
       "2                  anger                      {'anger': 0.9504784345626831}   \n",
       "3       [neutral, anger]  {'neutral': 0.5479668378829956, 'anger': 0.251...   \n",
       "4          [anger, fear]  {'anger': 0.4802381694316864, 'fear': 0.261093...   \n",
       "...                  ...                                                ...   \n",
       "2537               anger                      {'anger': 0.9745969176292419}   \n",
       "2538             disgust                    {'disgust': 0.9778687357902527}   \n",
       "2539            surprise                   {'surprise': 0.8578999042510986}   \n",
       "2540    [sadness, anger]  {'sadness': 0.4839746654033661, 'anger': 0.256...   \n",
       "2541      [neutral, joy]  {'neutral': 0.6780093312263489, 'joy': 0.12183...   \n",
       "\n",
       "            source_file  \n",
       "0      submission_0.csv  \n",
       "1      submission_0.csv  \n",
       "2      submission_0.csv  \n",
       "3      submission_0.csv  \n",
       "4      submission_0.csv  \n",
       "...                 ...  \n",
       "2537  submission_96.csv  \n",
       "2538  submission_96.csv  \n",
       "2539  submission_96.csv  \n",
       "2540  submission_96.csv  \n",
       "2541  submission_96.csv  \n",
       "\n",
       "[2542 rows x 14 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db87fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uoft_senti_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
