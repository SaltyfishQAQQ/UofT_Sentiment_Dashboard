{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c37ea9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries for Reddit API data collection\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import re\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "import string\n",
    "from pathlib import Path\n",
    "import os\n",
    "from pysentimiento import create_analyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\n",
    "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb0abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove all special characters from the text except for letters, numbers, and spaces.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def convert_emoji_to_text(text, emoji_wrapper=\"emoji\"):\n",
    "    \"\"\"\n",
    "    Converts emoji in the text to descriptive text.\n",
    "    \n",
    "    This function uses emoji.demojize() to replace any emoji with their\n",
    "    corresponding text (e.g., \"ðŸ˜„\" becomes \":smile:\"). It then wraps the\n",
    "    emoji description using the provided emoji_wrapper.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The original text containing emoji.\n",
    "        emoji_wrapper (str): A string that will be used to wrap the emoji text.\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with emojis converted to wrapped text.\n",
    "    \"\"\"\n",
    "    # Convert emojis to descriptive text (e.g., :smile:)\n",
    "    demojized = emoji.demojize(text)\n",
    "    # Define a wrapper string (e.g., \" emoji \")\n",
    "    wrapper = f\" {emoji_wrapper} \".replace(\"  \", \" \")\n",
    "    # Replace the demojized emoji pattern :emoji_name: with the wrapped emoji_name\n",
    "    # For example, \":smile:\" becomes \" emoji smile emoji \"\n",
    "    result = re.sub(r':([^:\\s]+):', lambda m: wrapper + m.group(1) + wrapper, demojized)\n",
    "    return result\n",
    "\n",
    "def is_valid_word(word):\n",
    "    \"\"\"\n",
    "    Returns True if the word is either:\n",
    "    - Composed solely of punctuation\n",
    "    - Contains at least one English letter (a-z or A-Z)\n",
    "    \"\"\"\n",
    "    # Keep if word is only punctuation\n",
    "    if all(ch in string.punctuation for ch in word):\n",
    "        return True\n",
    "    # Keep if the word contains at least one ASCII letter\n",
    "    if re.search(r'[a-zA-Z]', word):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_non_english(text):\n",
    "    \"\"\"\n",
    "    Splits the text into words and filters out any word that is not an English word,\n",
    "    an emoji, or punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if is_valid_word(word)]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 4. Convert created_utc to readable date format (YYYY-MM-DD)\n",
    "def convert_utc_to_readable(utc_timestamp):\n",
    "    \"\"\"\n",
    "    Convert UTC timestamp to a readable date format (YYYY-MM-DD).\n",
    "    \n",
    "    Args:\n",
    "        utc_timestamp (float): UTC timestamp in seconds since epoch.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted date string or empty string if input is NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Handle NaN values\n",
    "        if pd.isna(utc_timestamp):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to datetime object\n",
    "        dt = datetime.fromtimestamp(utc_timestamp)\n",
    "        \n",
    "        # Format as YYYY-MM-DD HH:MM:SS\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    except (ValueError, TypeError, OSError):\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95970d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(master_df, text_column='body'):\n",
    "    \"\"\"\n",
    "    Iterates through each text in the DataFrame, applies sentiment_analyzer,\n",
    "    and adds two columns: sentiment_prediction and sentiment_prob.\n",
    "    \n",
    "    Args:\n",
    "        master_df (pd.DataFrame): The DataFrame to analyze\n",
    "        text_column (str): The column containing text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added sentiment columns\n",
    "    \"\"\"\n",
    "    senti_pred = []\n",
    "    senti_prob = []\n",
    "    \n",
    "    for index, row in master_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        \n",
    "        if pd.isna(text) or text == \"\":\n",
    "            senti_pred.append(\"NEU\")\n",
    "            senti_prob.append({\"NEU\": 0.34, \"NEG\": 0.33, \"POS\": 0.33})\n",
    "        else:\n",
    "            result = sentiment_analyzer.predict(text)\n",
    "            senti_pred.append(result.output)\n",
    "            senti_prob.append(result.probas)\n",
    "    \n",
    "    master_df['sentiment_prediction'] = senti_pred\n",
    "    master_df['sentiment_prob'] = senti_prob\n",
    "    master_df['sentiment_score'] = master_df['sentiment_prob'].apply(lambda x: x.get('POS', 0) - x.get('NEG', 0))\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "def emotion_analysis(input_df, text_column='body'):\n",
    "    emotion_pred = []\n",
    "    emotion_prob = []\n",
    "    \n",
    "    for index, row in input_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        result = classifier(text)\n",
    "        emotion_pred.append(result[0]['label'])\n",
    "        emotion_prob.append({label['label']: label['score'] for label in result})\n",
    "\n",
    "    input_df['emotion_prediction'] = emotion_pred\n",
    "    input_df['emotion_prob'] = emotion_prob\n",
    "\n",
    "    return input_df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all preprocessing steps to a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # 1. Keep only posts with URLs starting with UofT comments\n",
    "    processed_df = processed_df[processed_df['url'].str.startswith('https://www.reddit.com/r/UofT/comments/')].reset_index(drop=True).copy()\n",
    "    \n",
    "    # 2. Clean body text: lowercase, strip, remove special chars except spaces\n",
    "    processed_df['body'] = processed_df['body'].str.lower()\n",
    "    processed_df['body'] = processed_df['body'].str.strip()\n",
    "    \n",
    "    # 3. Remove special characters except spaces\n",
    "    processed_df['body'] = processed_df['body'].apply(clean_text)\n",
    "    \n",
    "    # 4. Converts emoji in the text to descriptive text\n",
    "    processed_df['body'] = processed_df['body'].apply(convert_emoji_to_text)\n",
    "    \n",
    "    # 5. Filter out non-English words\n",
    "    processed_df['body'] = processed_df['body'].apply(filter_non_english)\n",
    "    \n",
    "    # 6. Convert created_utc to readable date format\n",
    "    processed_df['created_utc'] = processed_df['created_utc'].apply(convert_utc_to_readable)\n",
    "    \n",
    "    # 7. Preprocess the body column using pysentimiento (optional - currently commented out)\n",
    "    # processed_df['body'] = processed_df['body'].apply(preprocess_tweet)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def process_all_monthly_submissions(folder_path, senti=True, emo=True):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the monthly submissions folder.\n",
    "    Apply preprocessing and sentiment analysis to each file.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all processed submissions\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    csv_files.sort()  # Sort to process in order\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    all_processed_data = []\n",
    "    \n",
    "    for i, filename in enumerate(csv_files):\n",
    "        if filename.startswith('top_100_reddits_'):\n",
    "            continue  # Skip the combined file if it exists\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file {i+1}/{len(csv_files)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  - Loaded {len(df)} rows\")\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            processed_df = preprocess_dataframe(df)\n",
    "            print(f\"  - After filtering: {len(processed_df)} rows\")\n",
    "            \n",
    "            # Apply sentiment analysis\n",
    "            if len(processed_df) > 0:\n",
    "                if senti:\n",
    "                    processed_df = sentiment_analysis(processed_df, text_column='body')\n",
    "                    print(f\"  - Sentiment analysis completed\")\n",
    "\n",
    "                if emo:\n",
    "                    processed_df = emotion_analysis(processed_df, text_column='body')\n",
    "                    print(f\"  - Emotion analysis completed\")\n",
    "                \n",
    "                # Add source file information\n",
    "                processed_df['source_file'] = filename\n",
    "                \n",
    "                all_processed_data.append(processed_df)\n",
    "            else:\n",
    "                print(f\"  - No data remaining after filtering\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all processed data\n",
    "    if all_processed_data:\n",
    "        combined_df = pd.concat(all_processed_data, ignore_index=True)\n",
    "        print(f\"\\nProcessing complete! Combined dataset has {len(combined_df)} rows\")\n",
    "        return all_processed_data, combined_df\n",
    "    else:\n",
    "        print(\"\\nNo data was successfully processed\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# processed_monthly_data = process_all_monthly_submissions('individual_submissions_month')\n",
    "# processed_monthly_data.to_excel('processed_monthly_submissions_with_sentiment.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90e7de78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process all monthly submission CSV files...\n",
      "Found 98 CSV files to process\n",
      "Processing file 1/98: submission_0.csv\n",
      "  - Loaded 89 rows\n",
      "  - After filtering: 88 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 2/98: submission_1.csv\n",
      "  - Loaded 176 rows\n",
      "  - After filtering: 175 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 3/98: submission_10.csv\n",
      "  - Loaded 34 rows\n",
      "  - After filtering: 34 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 4/98: submission_11.csv\n",
      "  - Loaded 78 rows\n",
      "  - After filtering: 78 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 5/98: submission_12.csv\n",
      "  - Loaded 56 rows\n",
      "  - After filtering: 55 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 6/98: submission_13.csv\n",
      "  - Loaded 44 rows\n",
      "  - After filtering: 44 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 7/98: submission_14.csv\n",
      "  - Loaded 96 rows\n",
      "  - After filtering: 96 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 8/98: submission_15.csv\n",
      "  - Loaded 22 rows\n",
      "  - After filtering: 22 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 9/98: submission_16.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 10/98: submission_17.csv\n",
      "  - Loaded 36 rows\n",
      "  - After filtering: 36 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 11/98: submission_18.csv\n",
      "  - Loaded 65 rows\n",
      "  - After filtering: 64 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 12/98: submission_19.csv\n",
      "  - Loaded 87 rows\n",
      "  - After filtering: 87 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 13/98: submission_2.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 25 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 14/98: submission_20.csv\n",
      "  - Loaded 31 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 15/98: submission_21.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 16/98: submission_22.csv\n",
      "  - Loaded 47 rows\n",
      "  - After filtering: 46 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 17/98: submission_23.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 18/98: submission_24.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 19/98: submission_25.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 20/98: submission_26.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 21/98: submission_27.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 22/98: submission_28.csv\n",
      "  - Loaded 22 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 23/98: submission_29.csv\n",
      "  - Loaded 30 rows\n",
      "  - After filtering: 29 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 24/98: submission_3.csv\n",
      "  - Loaded 35 rows\n",
      "  - After filtering: 35 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 25/98: submission_30.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 26/98: submission_31.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 27/98: submission_32.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 28/98: submission_33.csv\n",
      "  - Loaded 56 rows\n",
      "  - After filtering: 56 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 29/98: submission_34.csv\n",
      "  - Loaded 94 rows\n",
      "  - After filtering: 94 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 30/98: submission_35.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 31/98: submission_36.csv\n",
      "  - Loaded 22 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 32/98: submission_37.csv\n",
      "  - Loaded 20 rows\n",
      "  - After filtering: 20 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 33/98: submission_38.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 34/98: submission_39.csv\n",
      "  - Loaded 38 rows\n",
      "  - After filtering: 38 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 35/98: submission_4.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 36/98: submission_40.csv\n",
      "  - Loaded 37 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 37/98: submission_41.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 38/98: submission_42.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 39/98: submission_43.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 40/98: submission_44.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 41/98: submission_45.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 42/98: submission_46.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 43/98: submission_47.csv\n",
      "  - Loaded 20 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 44/98: submission_48.csv\n",
      "  - Loaded 29 rows\n",
      "  - After filtering: 29 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 45/98: submission_49.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 46/98: submission_5.csv\n",
      "  - Loaded 44 rows\n",
      "  - After filtering: 44 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 47/98: submission_50.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 48/98: submission_51.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 49/98: submission_52.csv\n",
      "  - Loaded 29 rows\n",
      "  - After filtering: 29 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 50/98: submission_53.csv\n",
      "  - Loaded 53 rows\n",
      "  - After filtering: 53 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 51/98: submission_54.csv\n",
      "  - Loaded 42 rows\n",
      "  - After filtering: 42 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 52/98: submission_55.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 53/98: submission_56.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 54/98: submission_57.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 55/98: submission_58.csv\n",
      "  - Loaded 73 rows\n",
      "  - After filtering: 72 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 56/98: submission_59.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 57/98: submission_6.csv\n",
      "  - Loaded 82 rows\n",
      "  - After filtering: 82 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 58/98: submission_60.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 59/98: submission_61.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 60/98: submission_62.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 61/98: submission_63.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 62/98: submission_64.csv\n",
      "  - Loaded 28 rows\n",
      "  - After filtering: 28 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 63/98: submission_65.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 64/98: submission_66.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 65/98: submission_67.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 66/98: submission_68.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 67/98: submission_69.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 68/98: submission_7.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 69/98: submission_70.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 70/98: submission_71.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 71/98: submission_72.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 72/98: submission_73.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 73/98: submission_74.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 74/98: submission_75.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 18 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 75/98: submission_76.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 76/98: submission_77.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 1 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 77/98: submission_78.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 78/98: submission_79.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 79/98: submission_8.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 80/98: submission_80.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 81/98: submission_81.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 82/98: submission_82.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 83/98: submission_83.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 84/98: submission_84.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 85/98: submission_85.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 86/98: submission_86.csv\n",
      "  - Loaded 16 rows\n",
      "  - After filtering: 16 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 87/98: submission_87.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 88/98: submission_88.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 89/98: submission_89.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 90/98: submission_9.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 91/98: submission_90.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 92/98: submission_91.csv\n",
      "  - Loaded 22 rows\n",
      "  - After filtering: 22 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 93/98: submission_92.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 94/98: submission_93.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 95/98: submission_94.csv\n",
      "  - Loaded 36 rows\n",
      "  - After filtering: 36 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 96/98: submission_95.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "Processing file 97/98: submission_96.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 3 rows\n",
      "  - Sentiment analysis completed\n",
      "\n",
      "Processing complete! Combined dataset has 2567 rows\n",
      "\n",
      "=== PROCESSING SUMMARY ===\n",
      "Total processed rows: 2567\n",
      "Unique submissions: 97\n",
      "Date range: 2025-08-28 10:29:15 to 2025-09-26 22:56:08\n",
      "\n",
      "=== SENTIMENT DISTRIBUTION ===\n",
      "sentiment_prediction\n",
      "NEU    1333\n",
      "NEG     747\n",
      "POS     487\n",
      "Name: count, dtype: int64\n",
      "Sentiment percentages:\n",
      "  NEU: 51.9%\n",
      "  NEG: 29.1%\n",
      "  POS: 19.0%\n"
     ]
    }
   ],
   "source": [
    "# Process all monthly submissions\n",
    "print(\"Starting to process all monthly submission CSV files...\")\n",
    "today = datetime.now().strftime('%Y%m')\n",
    "folder_path = f'monthly_top100/{today}'\n",
    "all_processed_data, processed_monthly_data = process_all_monthly_submissions(folder_path, senti=True, emo=False)\n",
    "\n",
    "# Display summary statistics\n",
    "if len(processed_monthly_data) > 0:\n",
    "    print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
    "    print(f\"Total processed rows: {len(processed_monthly_data)}\")\n",
    "    print(f\"Unique submissions: {processed_monthly_data['submission_title'].nunique()}\")\n",
    "    print(f\"Date range: {processed_monthly_data['created_utc'].min()} to {processed_monthly_data['created_utc'].max()}\")\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    print(f\"\\n=== SENTIMENT DISTRIBUTION ===\")\n",
    "    sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    print(f\"Sentiment percentages:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(processed_monthly_data)) * 100\n",
    "        print(f\"  {sentiment}: {percentage:.1f}%\")\n",
    "else:\n",
    "    print(\"No data was processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6381a8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "today = datetime.now().strftime('%Y%m')\n",
    "file_name = Path(f'top_100_reddits_{today}.csv')\n",
    "processed_monthly_data.to_csv(folder_path / file_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e941a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a730e8db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment_prediction</th>\n",
       "      <th>sentiment_prob</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>buckbuck5645</td>\n",
       "      <td>nes2v1g</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 17:35:18</td>\n",
       "      <td>this might be one of the dumbest questions ive...</td>\n",
       "      <td>comment</td>\n",
       "      <td>271</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.978022575378418, 'NEU': 0.0189271867...</td>\n",
       "      <td>-0.974972</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>SNG_Blitzy</td>\n",
       "      <td>nesxtjv</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 20:29:13</td>\n",
       "      <td>its essentially the same thing but its crucial...</td>\n",
       "      <td>comment</td>\n",
       "      <td>132</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.02854382060468197, 'NEU': 0.94904333...</td>\n",
       "      <td>-0.006131</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>BromineFromine</td>\n",
       "      <td>nes8way</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 18:07:21</td>\n",
       "      <td>when you put a vengeful pedant in charge of yo...</td>\n",
       "      <td>comment</td>\n",
       "      <td>64</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.9388996362686157, 'NEU': 0.057808786...</td>\n",
       "      <td>-0.935608</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>onlyonequickquestion</td>\n",
       "      <td>neskxvd</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 19:15:21</td>\n",
       "      <td>well maybe in a technical or legal sense there...</td>\n",
       "      <td>comment</td>\n",
       "      <td>39</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.8017464280128479, 'NEU': 0.194327071...</td>\n",
       "      <td>-0.797820</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>these types of questions are actually so dumb ...</td>\n",
       "      <td>BackgroundBench530</td>\n",
       "      <td>netfy6p</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1njph3e...</td>\n",
       "      <td>2025-09-17 22:14:05</td>\n",
       "      <td>im a criminologist even the correct answer is ...</td>\n",
       "      <td>comment</td>\n",
       "      <td>20</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.3340751528739929, 'NEU': 0.637177526...</td>\n",
       "      <td>-0.305328</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>the eng kids seem to have so much fun together...</td>\n",
       "      <td>moneyhalter</td>\n",
       "      <td>nc4pxub</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1n71a1h...</td>\n",
       "      <td>2025-09-02 23:12:43</td>\n",
       "      <td>this years 2t9 cohort was insane i hear much p...</td>\n",
       "      <td>comment</td>\n",
       "      <td>6</td>\n",
       "      <td>POS</td>\n",
       "      <td>{'NEG': 0.0014727545203641057, 'NEU': 0.017684...</td>\n",
       "      <td>0.979370</td>\n",
       "      <td>submission_95.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2563</th>\n",
       "      <td>the eng kids seem to have so much fun together...</td>\n",
       "      <td>Starboy-XO17</td>\n",
       "      <td>nc6gd8p</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1n71a1h...</td>\n",
       "      <td>2025-09-03 08:05:21</td>\n",
       "      <td>no one does that stop spreading bullshit we do...</td>\n",
       "      <td>comment</td>\n",
       "      <td>21</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.5473296642303467, 'NEU': 0.438754767...</td>\n",
       "      <td>-0.533414</td>\n",
       "      <td>submission_95.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2564</th>\n",
       "      <td>I made a website that has every event happenin...</td>\n",
       "      <td>Dchen_08</td>\n",
       "      <td>nb5b50e</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1n2f56q...</td>\n",
       "      <td>2025-08-28 11:20:30</td>\n",
       "      <td>if youre a club admin and are interested fill ...</td>\n",
       "      <td>comment</td>\n",
       "      <td>3</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.0021603929344564676, 'NEU': 0.786184...</td>\n",
       "      <td>0.209495</td>\n",
       "      <td>submission_96.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2565</th>\n",
       "      <td>I made a website that has every event happenin...</td>\n",
       "      <td>No-Bee8635</td>\n",
       "      <td>nb5snl7</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1n2f56q...</td>\n",
       "      <td>2025-08-28 12:43:25</td>\n",
       "      <td>ive been using camel for the past few weeks no...</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>POS</td>\n",
       "      <td>{'NEG': 0.0010805773781612515, 'NEU': 0.063199...</td>\n",
       "      <td>0.934639</td>\n",
       "      <td>submission_96.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2566</th>\n",
       "      <td>I made a website that has every event happenin...</td>\n",
       "      <td>sadmanca</td>\n",
       "      <td>nb7pjer</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1n2f56q...</td>\n",
       "      <td>2025-08-28 18:16:11</td>\n",
       "      <td>this is actually super cool instant sign up fo...</td>\n",
       "      <td>comment</td>\n",
       "      <td>2</td>\n",
       "      <td>POS</td>\n",
       "      <td>{'NEG': 0.0016407547518610954, 'NEU': 0.009461...</td>\n",
       "      <td>0.987257</td>\n",
       "      <td>submission_96.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2567 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       submission_title                author  \\\n",
       "0     these types of questions are actually so dumb ...          buckbuck5645   \n",
       "1     these types of questions are actually so dumb ...            SNG_Blitzy   \n",
       "2     these types of questions are actually so dumb ...        BromineFromine   \n",
       "3     these types of questions are actually so dumb ...  onlyonequickquestion   \n",
       "4     these types of questions are actually so dumb ...    BackgroundBench530   \n",
       "...                                                 ...                   ...   \n",
       "2562  the eng kids seem to have so much fun together...           moneyhalter   \n",
       "2563  the eng kids seem to have so much fun together...          Starboy-XO17   \n",
       "2564  I made a website that has every event happenin...              Dchen_08   \n",
       "2565  I made a website that has every event happenin...            No-Bee8635   \n",
       "2566  I made a website that has every event happenin...              sadmanca   \n",
       "\n",
       "           id                                                url  \\\n",
       "0     nes2v1g  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "1     nesxtjv  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "2     nes8way  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "3     neskxvd  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "4     netfy6p  https://www.reddit.com/r/UofT/comments/1njph3e...   \n",
       "...       ...                                                ...   \n",
       "2562  nc4pxub  https://www.reddit.com/r/UofT/comments/1n71a1h...   \n",
       "2563  nc6gd8p  https://www.reddit.com/r/UofT/comments/1n71a1h...   \n",
       "2564  nb5b50e  https://www.reddit.com/r/UofT/comments/1n2f56q...   \n",
       "2565  nb5snl7  https://www.reddit.com/r/UofT/comments/1n2f56q...   \n",
       "2566  nb7pjer  https://www.reddit.com/r/UofT/comments/1n2f56q...   \n",
       "\n",
       "              created_utc                                               body  \\\n",
       "0     2025-09-17 17:35:18  this might be one of the dumbest questions ive...   \n",
       "1     2025-09-17 20:29:13  its essentially the same thing but its crucial...   \n",
       "2     2025-09-17 18:07:21  when you put a vengeful pedant in charge of yo...   \n",
       "3     2025-09-17 19:15:21  well maybe in a technical or legal sense there...   \n",
       "4     2025-09-17 22:14:05  im a criminologist even the correct answer is ...   \n",
       "...                   ...                                                ...   \n",
       "2562  2025-09-02 23:12:43  this years 2t9 cohort was insane i hear much p...   \n",
       "2563  2025-09-03 08:05:21  no one does that stop spreading bullshit we do...   \n",
       "2564  2025-08-28 11:20:30  if youre a club admin and are interested fill ...   \n",
       "2565  2025-08-28 12:43:25  ive been using camel for the past few weeks no...   \n",
       "2566  2025-08-28 18:16:11  this is actually super cool instant sign up fo...   \n",
       "\n",
       "         type  score sentiment_prediction  \\\n",
       "0     comment    271                  NEG   \n",
       "1     comment    132                  NEU   \n",
       "2     comment     64                  NEG   \n",
       "3     comment     39                  NEG   \n",
       "4     comment     20                  NEU   \n",
       "...       ...    ...                  ...   \n",
       "2562  comment      6                  POS   \n",
       "2563  comment     21                  NEG   \n",
       "2564  comment      3                  NEU   \n",
       "2565  comment      2                  POS   \n",
       "2566  comment      2                  POS   \n",
       "\n",
       "                                         sentiment_prob  sentiment_score  \\\n",
       "0     {'NEG': 0.978022575378418, 'NEU': 0.0189271867...        -0.974972   \n",
       "1     {'NEG': 0.02854382060468197, 'NEU': 0.94904333...        -0.006131   \n",
       "2     {'NEG': 0.9388996362686157, 'NEU': 0.057808786...        -0.935608   \n",
       "3     {'NEG': 0.8017464280128479, 'NEU': 0.194327071...        -0.797820   \n",
       "4     {'NEG': 0.3340751528739929, 'NEU': 0.637177526...        -0.305328   \n",
       "...                                                 ...              ...   \n",
       "2562  {'NEG': 0.0014727545203641057, 'NEU': 0.017684...         0.979370   \n",
       "2563  {'NEG': 0.5473296642303467, 'NEU': 0.438754767...        -0.533414   \n",
       "2564  {'NEG': 0.0021603929344564676, 'NEU': 0.786184...         0.209495   \n",
       "2565  {'NEG': 0.0010805773781612515, 'NEU': 0.063199...         0.934639   \n",
       "2566  {'NEG': 0.0016407547518610954, 'NEU': 0.009461...         0.987257   \n",
       "\n",
       "            source_file  \n",
       "0      submission_0.csv  \n",
       "1      submission_0.csv  \n",
       "2      submission_0.csv  \n",
       "3      submission_0.csv  \n",
       "4      submission_0.csv  \n",
       "...                 ...  \n",
       "2562  submission_95.csv  \n",
       "2563  submission_95.csv  \n",
       "2564  submission_96.csv  \n",
       "2565  submission_96.csv  \n",
       "2566  submission_96.csv  \n",
       "\n",
       "[2567 rows x 12 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db87fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uoft_senti_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
