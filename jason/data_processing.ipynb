{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c37ea9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/uoft_senti_db/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries for Reddit API data collection\n",
    "import praw  # For making HTTP requests to Reddit API\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import re\n",
    "from datetime import datetime\n",
    "import emoji\n",
    "import string\n",
    "import os\n",
    "from pysentimiento import create_analyzer\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"michellejieli/emotion_text_classifier\")\n",
    "sentiment_analyzer = create_analyzer(task=\"sentiment\", lang=\"en\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3eb0abee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Remove all special characters from the text except for letters, numbers, and spaces.\n",
    "    \"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "\n",
    "def convert_emoji_to_text(text, emoji_wrapper=\"emoji\"):\n",
    "    \"\"\"\n",
    "    Converts emoji in the text to descriptive text.\n",
    "    \n",
    "    This function uses emoji.demojize() to replace any emoji with their\n",
    "    corresponding text (e.g., \"ðŸ˜„\" becomes \":smile:\"). It then wraps the\n",
    "    emoji description using the provided emoji_wrapper.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The original text containing emoji.\n",
    "        emoji_wrapper (str): A string that will be used to wrap the emoji text.\n",
    "        \n",
    "    Returns:\n",
    "        str: The text with emojis converted to wrapped text.\n",
    "    \"\"\"\n",
    "    # Convert emojis to descriptive text (e.g., :smile:)\n",
    "    demojized = emoji.demojize(text)\n",
    "    # Define a wrapper string (e.g., \" emoji \")\n",
    "    wrapper = f\" {emoji_wrapper} \".replace(\"  \", \" \")\n",
    "    # Replace the demojized emoji pattern :emoji_name: with the wrapped emoji_name\n",
    "    # For example, \":smile:\" becomes \" emoji smile emoji \"\n",
    "    result = re.sub(r':([^:\\s]+):', lambda m: wrapper + m.group(1) + wrapper, demojized)\n",
    "    return result\n",
    "\n",
    "def is_valid_word(word):\n",
    "    \"\"\"\n",
    "    Returns True if the word is either:\n",
    "    - Composed solely of punctuation\n",
    "    - Contains at least one English letter (a-z or A-Z)\n",
    "    \"\"\"\n",
    "    # Keep if word is only punctuation\n",
    "    if all(ch in string.punctuation for ch in word):\n",
    "        return True\n",
    "    # Keep if the word contains at least one ASCII letter\n",
    "    if re.search(r'[a-zA-Z]', word):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def filter_non_english(text):\n",
    "    \"\"\"\n",
    "    Splits the text into words and filters out any word that is not an English word,\n",
    "    an emoji, or punctuation.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The input text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The cleaned text.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if is_valid_word(word)]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# 4. Convert created_utc to readable date format (YYYY-MM-DD)\n",
    "def convert_utc_to_readable(utc_timestamp):\n",
    "    \"\"\"\n",
    "    Convert UTC timestamp to a readable date format (YYYY-MM-DD).\n",
    "    \n",
    "    Args:\n",
    "        utc_timestamp (float): UTC timestamp in seconds since epoch.\n",
    "        \n",
    "    Returns:\n",
    "        str: Formatted date string or empty string if input is NaN.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Handle NaN values\n",
    "        if pd.isna(utc_timestamp):\n",
    "            return \"\"\n",
    "        \n",
    "        # Convert to datetime object\n",
    "        dt = datetime.fromtimestamp(utc_timestamp)\n",
    "        \n",
    "        # Format as YYYY-MM-DD HH:MM:SS\n",
    "        return dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    except (ValueError, TypeError, OSError):\n",
    "        return \"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95970d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_analysis(master_df, text_column='body'):\n",
    "    \"\"\"\n",
    "    Iterates through each text in the DataFrame, applies sentiment_analyzer,\n",
    "    and adds two columns: sentiment_prediction and sentiment_prob.\n",
    "    \n",
    "    Args:\n",
    "        master_df (pd.DataFrame): The DataFrame to analyze\n",
    "        text_column (str): The column containing text to analyze\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with added sentiment columns\n",
    "    \"\"\"\n",
    "    senti_pred = []\n",
    "    senti_prob = []\n",
    "    \n",
    "    for index, row in master_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        \n",
    "        if pd.isna(text) or text == \"\":\n",
    "            senti_pred.append(\"NEU\")\n",
    "            senti_prob.append({\"NEU\": 0.34, \"NEG\": 0.33, \"POS\": 0.33})\n",
    "        else:\n",
    "            result = sentiment_analyzer.predict(text)\n",
    "            senti_pred.append(result.output)\n",
    "            senti_prob.append(result.probas)\n",
    "    \n",
    "    master_df['sentiment_prediction'] = senti_pred\n",
    "    master_df['sentiment_prob'] = senti_prob\n",
    "    \n",
    "    return master_df\n",
    "\n",
    "def emotion_analysis(input_df, text_column='body'):\n",
    "    emotion_pred = []\n",
    "    emotion_prob = []\n",
    "    \n",
    "    for index, row in input_df.iterrows():\n",
    "        text = row[text_column]\n",
    "        result = classifier(text)\n",
    "        emotion_pred.append(result[0]['label'])\n",
    "        emotion_prob.append({label['label']: label['score'] for label in result})\n",
    "\n",
    "    input_df['emotion_prediction'] = emotion_pred\n",
    "    input_df['emotion_prob'] = emotion_prob\n",
    "\n",
    "    return input_df\n",
    "\n",
    "def preprocess_dataframe(df):\n",
    "    \"\"\"\n",
    "    Apply all preprocessing steps to a DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame to preprocess\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: The preprocessed DataFrame\n",
    "    \"\"\"\n",
    "    # Make a copy to avoid modifying the original\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # 1. Keep only posts with URLs starting with UofT comments\n",
    "    processed_df = processed_df[processed_df['url'].str.startswith('https://www.reddit.com/r/UofT/comments/')].reset_index(drop=True).copy()\n",
    "    \n",
    "    # 2. Clean body text: lowercase, strip, remove special chars except spaces\n",
    "    processed_df['body'] = processed_df['body'].str.lower()\n",
    "    processed_df['body'] = processed_df['body'].str.strip()\n",
    "    \n",
    "    # 3. Remove special characters except spaces\n",
    "    processed_df['body'] = processed_df['body'].apply(clean_text)\n",
    "    \n",
    "    # 4. Converts emoji in the text to descriptive text\n",
    "    processed_df['body'] = processed_df['body'].apply(convert_emoji_to_text)\n",
    "    \n",
    "    # 5. Filter out non-English words\n",
    "    processed_df['body'] = processed_df['body'].apply(filter_non_english)\n",
    "    \n",
    "    # 6. Convert created_utc to readable date format\n",
    "    processed_df['created_utc'] = processed_df['created_utc'].apply(convert_utc_to_readable)\n",
    "    \n",
    "    # 7. Preprocess the body column using pysentimiento (optional - currently commented out)\n",
    "    # processed_df['body'] = processed_df['body'].apply(preprocess_tweet)\n",
    "    \n",
    "    return processed_df\n",
    "\n",
    "\n",
    "def process_all_monthly_submissions(folder_path='individual_submissions_month'):\n",
    "    \"\"\"\n",
    "    Process all CSV files in the monthly submissions folder.\n",
    "    Apply preprocessing and sentiment analysis to each file.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing CSV files\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame with all processed submissions\n",
    "    \"\"\"\n",
    "    # Get list of all CSV files in the folder\n",
    "    csv_files = [f for f in os.listdir(folder_path) if f.endswith('.csv')]\n",
    "    csv_files.sort()  # Sort to process in order\n",
    "    \n",
    "    print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "    \n",
    "    all_processed_data = []\n",
    "    \n",
    "    for i, filename in enumerate(csv_files):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        print(f\"Processing file {i+1}/{len(csv_files)}: {filename}\")\n",
    "        \n",
    "        try:\n",
    "            # Load the CSV file\n",
    "            df = pd.read_csv(file_path)\n",
    "            print(f\"  - Loaded {len(df)} rows\")\n",
    "            \n",
    "            # Apply preprocessing\n",
    "            processed_df = preprocess_dataframe(df)\n",
    "            print(f\"  - After filtering: {len(processed_df)} rows\")\n",
    "            \n",
    "            # Apply sentiment analysis\n",
    "            if len(processed_df) > 0:\n",
    "                processed_df = sentiment_analysis(processed_df, text_column='body')\n",
    "                print(f\"  - Sentiment analysis completed\")\n",
    "\n",
    "                processed_df = emotion_analysis(processed_df, text_column='body')\n",
    "                print(f\"  - Emotion analysis completed\")\n",
    "                \n",
    "                # Add source file information\n",
    "                processed_df['source_file'] = filename\n",
    "                \n",
    "                all_processed_data.append(processed_df)\n",
    "            else:\n",
    "                print(f\"  - No data remaining after filtering\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {filename}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all processed data\n",
    "    if all_processed_data:\n",
    "        combined_df = pd.concat(all_processed_data, ignore_index=True)\n",
    "        print(f\"\\nProcessing complete! Combined dataset has {len(combined_df)} rows\")\n",
    "        return all_processed_data, combined_df\n",
    "    else:\n",
    "        print(\"\\nNo data was successfully processed\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# processed_monthly_data = process_all_monthly_submissions('individual_submissions_month')\n",
    "# processed_monthly_data.to_excel('processed_monthly_submissions_with_sentiment.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90e7de78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to process all monthly submission CSV files...\n",
      "Found 100 CSV files to process\n",
      "Processing file 1/100: submission_0.csv\n",
      "  - Loaded 59 rows\n",
      "  - After filtering: 58 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 2/100: submission_1.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 3/100: submission_10.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 4/100: submission_11.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 5/100: submission_12.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 27 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 6/100: submission_13.csv\n",
      "  - Loaded 32 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 7/100: submission_14.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 8/100: submission_15.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 9/100: submission_16.csv\n",
      "  - Loaded 39 rows\n",
      "  - After filtering: 38 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 10/100: submission_17.csv\n",
      "  - Loaded 45 rows\n",
      "  - After filtering: 45 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 11/100: submission_18.csv\n",
      "  - Loaded 23 rows\n",
      "  - After filtering: 23 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 12/100: submission_19.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 13/100: submission_2.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 32 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 14/100: submission_20.csv\n",
      "  - Loaded 25 rows\n",
      "  - After filtering: 25 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 15/100: submission_21.csv\n",
      "  - Loaded 28 rows\n",
      "  - After filtering: 27 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 16/100: submission_22.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 17/100: submission_23.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 18/100: submission_24.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 19/100: submission_25.csv\n",
      "  - Loaded 50 rows\n",
      "  - After filtering: 49 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 20/100: submission_26.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 21/100: submission_27.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 22/100: submission_28.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 23/100: submission_29.csv\n",
      "  - Loaded 24 rows\n",
      "  - After filtering: 24 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 24/100: submission_3.csv\n",
      "  - Loaded 93 rows\n",
      "  - After filtering: 93 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_3.csv: The expanded size of the tensor (516) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 516].  Tensor sizes: [1, 514]\n",
      "Processing file 25/100: submission_30.csv\n",
      "  - Loaded 31 rows\n",
      "  - After filtering: 31 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 26/100: submission_31.csv\n",
      "  - Loaded 48 rows\n",
      "  - After filtering: 48 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 27/100: submission_32.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 28/100: submission_33.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 29/100: submission_34.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_34.csv: The expanded size of the tensor (675) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 675].  Tensor sizes: [1, 514]\n",
      "Processing file 30/100: submission_35.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 31/100: submission_36.csv\n",
      "  - Loaded 30 rows\n",
      "  - After filtering: 30 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 32/100: submission_37.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 33/100: submission_38.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 34/100: submission_39.csv\n",
      "  - Loaded 49 rows\n",
      "  - After filtering: 49 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 35/100: submission_4.csv\n",
      "  - Loaded 71 rows\n",
      "  - After filtering: 71 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_4.csv: The expanded size of the tensor (635) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 635].  Tensor sizes: [1, 514]\n",
      "Processing file 36/100: submission_40.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 37/100: submission_41.csv\n",
      "  - Loaded 38 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 38/100: submission_42.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 39/100: submission_43.csv\n",
      "  - Loaded 59 rows\n",
      "  - After filtering: 59 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 40/100: submission_44.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 41/100: submission_45.csv\n",
      "  - Loaded 42 rows\n",
      "  - After filtering: 42 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 42/100: submission_46.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 43/100: submission_47.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 44/100: submission_48.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 45/100: submission_49.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_49.csv: The expanded size of the tensor (547) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 547].  Tensor sizes: [1, 514]\n",
      "Processing file 46/100: submission_5.csv\n",
      "  - Loaded 55 rows\n",
      "  - After filtering: 55 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_5.csv: The expanded size of the tensor (1145) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1145].  Tensor sizes: [1, 514]\n",
      "Processing file 47/100: submission_50.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 48/100: submission_51.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 49/100: submission_52.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 50/100: submission_53.csv\n",
      "  - Loaded 41 rows\n",
      "  - After filtering: 41 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_53.csv: The expanded size of the tensor (620) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 620].  Tensor sizes: [1, 514]\n",
      "Processing file 51/100: submission_54.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 52/100: submission_55.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 53/100: submission_56.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 54/100: submission_57.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 3 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 55/100: submission_58.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 56/100: submission_59.csv\n",
      "  - Loaded 8 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 57/100: submission_6.csv\n",
      "  - Loaded 52 rows\n",
      "  - After filtering: 51 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 58/100: submission_60.csv\n",
      "  - Loaded 27 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 59/100: submission_61.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 60/100: submission_62.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 61/100: submission_63.csv\n",
      "  - Loaded 19 rows\n",
      "  - After filtering: 19 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 62/100: submission_64.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 63/100: submission_65.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 64/100: submission_66.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_66.csv: The expanded size of the tensor (605) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 605].  Tensor sizes: [1, 514]\n",
      "Processing file 65/100: submission_67.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 66/100: submission_68.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 67/100: submission_69.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 68/100: submission_7.csv\n",
      "  - Loaded 35 rows\n",
      "  - After filtering: 35 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 69/100: submission_70.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 70/100: submission_71.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 71/100: submission_72.csv\n",
      "  - Loaded 5 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 72/100: submission_73.csv\n",
      "  - Loaded 11 rows\n",
      "  - After filtering: 11 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 73/100: submission_74.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 74/100: submission_75.csv\n",
      "  - Loaded 3 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 75/100: submission_76.csv\n",
      "  - Loaded 17 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 76/100: submission_77.csv\n",
      "  - Loaded 33 rows\n",
      "  - After filtering: 33 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_77.csv: The expanded size of the tensor (1008) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 1008].  Tensor sizes: [1, 514]\n",
      "Processing file 77/100: submission_78.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_78.csv: The expanded size of the tensor (527) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 527].  Tensor sizes: [1, 514]\n",
      "Processing file 78/100: submission_79.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 79/100: submission_8.csv\n",
      "  - Loaded 37 rows\n",
      "  - After filtering: 37 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_8.csv: The expanded size of the tensor (602) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 602].  Tensor sizes: [1, 514]\n",
      "Processing file 80/100: submission_80.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 81/100: submission_81.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 82/100: submission_82.csv\n",
      "  - Loaded 12 rows\n",
      "  - After filtering: 12 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 83/100: submission_83.csv\n",
      "  - Loaded 13 rows\n",
      "  - After filtering: 13 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 84/100: submission_84.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 8 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 85/100: submission_85.csv\n",
      "  - Loaded 21 rows\n",
      "  - After filtering: 21 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_85.csv: The expanded size of the tensor (572) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 572].  Tensor sizes: [1, 514]\n",
      "Processing file 86/100: submission_86.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 87/100: submission_87.csv\n",
      "  - Loaded 2 rows\n",
      "  - After filtering: 2 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 88/100: submission_88.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 5 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 89/100: submission_89.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 90/100: submission_9.csv\n",
      "  - Loaded 38 rows\n",
      "  - After filtering: 38 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 91/100: submission_90.csv\n",
      "  - Loaded 7 rows\n",
      "  - After filtering: 7 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 92/100: submission_91.csv\n",
      "  - Loaded 9 rows\n",
      "  - After filtering: 9 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 93/100: submission_92.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 94/100: submission_93.csv\n",
      "  - Loaded 10 rows\n",
      "  - After filtering: 10 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 95/100: submission_94.csv\n",
      "  - Loaded 4 rows\n",
      "  - After filtering: 4 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 96/100: submission_95.csv\n",
      "  - Loaded 15 rows\n",
      "  - After filtering: 15 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 97/100: submission_96.csv\n",
      "  - Loaded 26 rows\n",
      "  - After filtering: 26 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Error processing submission_96.csv: The expanded size of the tensor (703) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [1, 703].  Tensor sizes: [1, 514]\n",
      "Processing file 98/100: submission_97.csv\n",
      "  - Loaded 14 rows\n",
      "  - After filtering: 14 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 99/100: submission_98.csv\n",
      "  - Loaded 6 rows\n",
      "  - After filtering: 6 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "Processing file 100/100: submission_99.csv\n",
      "  - Loaded 18 rows\n",
      "  - After filtering: 17 rows\n",
      "  - Sentiment analysis completed\n",
      "  - Emotion analysis completed\n",
      "\n",
      "Processing complete! Combined dataset has 1592 rows\n",
      "\n",
      "=== PROCESSING SUMMARY ===\n",
      "Total processed rows: 1592\n",
      "Unique submissions: 88\n",
      "Date range: 2025-07-07 23:13:42 to 2025-08-06 22:01:39\n",
      "\n",
      "=== SENTIMENT DISTRIBUTION ===\n",
      "sentiment_prediction\n",
      "NEU    882\n",
      "NEG    389\n",
      "POS    321\n",
      "Name: count, dtype: int64\n",
      "Sentiment percentages:\n",
      "  NEU: 55.4%\n",
      "  NEG: 24.4%\n",
      "  POS: 20.2%\n"
     ]
    }
   ],
   "source": [
    "# Process all monthly submissions\n",
    "print(\"Starting to process all monthly submission CSV files...\")\n",
    "all_processed_data, processed_monthly_data = process_all_monthly_submissions('individual_submissions_month')\n",
    "\n",
    "# Display summary statistics\n",
    "if len(processed_monthly_data) > 0:\n",
    "    print(f\"\\n=== PROCESSING SUMMARY ===\")\n",
    "    print(f\"Total processed rows: {len(processed_monthly_data)}\")\n",
    "    print(f\"Unique submissions: {processed_monthly_data['submission_title'].nunique()}\")\n",
    "    print(f\"Date range: {processed_monthly_data['created_utc'].min()} to {processed_monthly_data['created_utc'].max()}\")\n",
    "    \n",
    "    # Sentiment distribution\n",
    "    print(f\"\\n=== SENTIMENT DISTRIBUTION ===\")\n",
    "    sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n",
    "    print(sentiment_counts)\n",
    "    print(f\"Sentiment percentages:\")\n",
    "    for sentiment, count in sentiment_counts.items():\n",
    "        percentage = (count / len(processed_monthly_data)) * 100\n",
    "        print(f\"  {sentiment}: {percentage:.1f}%\")\n",
    "else:\n",
    "    print(\"No data was processed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6381a8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>submission_title</th>\n",
       "      <th>author</th>\n",
       "      <th>id</th>\n",
       "      <th>url</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>body</th>\n",
       "      <th>type</th>\n",
       "      <th>score</th>\n",
       "      <th>sentiment_prediction</th>\n",
       "      <th>sentiment_prob</th>\n",
       "      <th>emotion_prediction</th>\n",
       "      <th>emotion_prob</th>\n",
       "      <th>source_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Why is UofT using AI?? Could they not have fou...</td>\n",
       "      <td>internetMujahideen</td>\n",
       "      <td>n5k4y1g</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1mb6vic...</td>\n",
       "      <td>2025-07-28 01:14:54</td>\n",
       "      <td>they dont make diverse utopian friend groups s...</td>\n",
       "      <td>comment</td>\n",
       "      <td>316</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.7034717202186584, 'NEU': 0.291085332...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.6760635375976562}</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is UofT using AI?? Could they not have fou...</td>\n",
       "      <td>random_name_245</td>\n",
       "      <td>n5kzzls</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1mb6vic...</td>\n",
       "      <td>2025-07-28 06:07:07</td>\n",
       "      <td>lol i wonder what the prompt was like have stu...</td>\n",
       "      <td>comment</td>\n",
       "      <td>116</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.16222292184829712, 'NEU': 0.65833210...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.9531378149986267}</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Why is UofT using AI?? Could they not have fou...</td>\n",
       "      <td>ihatedougford</td>\n",
       "      <td>n5k06ru</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1mb6vic...</td>\n",
       "      <td>2025-07-28 00:35:56</td>\n",
       "      <td>classic university college the cheapest and la...</td>\n",
       "      <td>comment</td>\n",
       "      <td>115</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.8319604992866516, 'NEU': 0.161235094...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.9534212350845337}</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Why is UofT using AI?? Could they not have fou...</td>\n",
       "      <td>BabaYagaTO</td>\n",
       "      <td>n5n0g1d</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1mb6vic...</td>\n",
       "      <td>2025-07-28 13:00:11</td>\n",
       "      <td>and now its gone httpswwwucutorontocaprogramss...</td>\n",
       "      <td>comment</td>\n",
       "      <td>16</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.3041565418243408, 'NEU': 0.678175747...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.9478220343589783}</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Why is UofT using AI?? Could they not have fou...</td>\n",
       "      <td>CaptainKoreana</td>\n",
       "      <td>n5kqi34</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1mb6vic...</td>\n",
       "      <td>2025-07-28 04:36:00</td>\n",
       "      <td>yikes uc</td>\n",
       "      <td>comment</td>\n",
       "      <td>14</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.7929663062095642, 'NEU': 0.183752104...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.9800550937652588}</td>\n",
       "      <td>submission_0.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>Is this scam? 111111111111111Â¹1111111111111111...</td>\n",
       "      <td>Potential_Fee4153</td>\n",
       "      <td>n75x28d</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1miqnu1...</td>\n",
       "      <td>2025-08-05 22:46:02</td>\n",
       "      <td>reason i thought it was scam cuz i already did...</td>\n",
       "      <td>comment</td>\n",
       "      <td>3</td>\n",
       "      <td>NEG</td>\n",
       "      <td>{'NEG': 0.5585876107215881, 'NEU': 0.434544444...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.8371781706809998}</td>\n",
       "      <td>submission_99.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1588</th>\n",
       "      <td>Is this scam? 111111111111111Â¹1111111111111111...</td>\n",
       "      <td>DeepGas4538</td>\n",
       "      <td>n75yhwt</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1miqnu1...</td>\n",
       "      <td>2025-08-05 22:54:44</td>\n",
       "      <td>perhaps you misread</td>\n",
       "      <td>comment</td>\n",
       "      <td>11</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.3851972818374634, 'NEU': 0.596996665...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.851717472076416}</td>\n",
       "      <td>submission_99.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1589</th>\n",
       "      <td>Is this scam? 111111111111111Â¹1111111111111111...</td>\n",
       "      <td>crud_lover</td>\n",
       "      <td>n761s49</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1miqnu1...</td>\n",
       "      <td>2025-08-05 23:15:36</td>\n",
       "      <td>all students are required to enroll with duo b...</td>\n",
       "      <td>comment</td>\n",
       "      <td>-9</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.09463470429182053, 'NEU': 0.89856177...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.9491429328918457}</td>\n",
       "      <td>submission_99.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1590</th>\n",
       "      <td>Is this scam? 111111111111111Â¹1111111111111111...</td>\n",
       "      <td>132ads</td>\n",
       "      <td>n76438p</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1miqnu1...</td>\n",
       "      <td>2025-08-05 23:30:46</td>\n",
       "      <td>double check the first comment you replied to</td>\n",
       "      <td>comment</td>\n",
       "      <td>11</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.012451458722352982, 'NEU': 0.9616963...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.9694627523422241}</td>\n",
       "      <td>submission_99.csv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1591</th>\n",
       "      <td>Is this scam? 111111111111111Â¹1111111111111111...</td>\n",
       "      <td>postmodern_girls</td>\n",
       "      <td>n764eqs</td>\n",
       "      <td>https://www.reddit.com/r/UofT/comments/1miqnu1...</td>\n",
       "      <td>2025-08-05 23:32:53</td>\n",
       "      <td>duo is mfa</td>\n",
       "      <td>comment</td>\n",
       "      <td>7</td>\n",
       "      <td>NEU</td>\n",
       "      <td>{'NEG': 0.012019496411085129, 'NEU': 0.9482759...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neutral': 0.9703196883201599}</td>\n",
       "      <td>submission_99.csv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1592 rows Ã— 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       submission_title              author  \\\n",
       "0     Why is UofT using AI?? Could they not have fou...  internetMujahideen   \n",
       "1     Why is UofT using AI?? Could they not have fou...     random_name_245   \n",
       "2     Why is UofT using AI?? Could they not have fou...       ihatedougford   \n",
       "3     Why is UofT using AI?? Could they not have fou...          BabaYagaTO   \n",
       "4     Why is UofT using AI?? Could they not have fou...      CaptainKoreana   \n",
       "...                                                 ...                 ...   \n",
       "1587  Is this scam? 111111111111111Â¹1111111111111111...   Potential_Fee4153   \n",
       "1588  Is this scam? 111111111111111Â¹1111111111111111...         DeepGas4538   \n",
       "1589  Is this scam? 111111111111111Â¹1111111111111111...          crud_lover   \n",
       "1590  Is this scam? 111111111111111Â¹1111111111111111...              132ads   \n",
       "1591  Is this scam? 111111111111111Â¹1111111111111111...    postmodern_girls   \n",
       "\n",
       "           id                                                url  \\\n",
       "0     n5k4y1g  https://www.reddit.com/r/UofT/comments/1mb6vic...   \n",
       "1     n5kzzls  https://www.reddit.com/r/UofT/comments/1mb6vic...   \n",
       "2     n5k06ru  https://www.reddit.com/r/UofT/comments/1mb6vic...   \n",
       "3     n5n0g1d  https://www.reddit.com/r/UofT/comments/1mb6vic...   \n",
       "4     n5kqi34  https://www.reddit.com/r/UofT/comments/1mb6vic...   \n",
       "...       ...                                                ...   \n",
       "1587  n75x28d  https://www.reddit.com/r/UofT/comments/1miqnu1...   \n",
       "1588  n75yhwt  https://www.reddit.com/r/UofT/comments/1miqnu1...   \n",
       "1589  n761s49  https://www.reddit.com/r/UofT/comments/1miqnu1...   \n",
       "1590  n76438p  https://www.reddit.com/r/UofT/comments/1miqnu1...   \n",
       "1591  n764eqs  https://www.reddit.com/r/UofT/comments/1miqnu1...   \n",
       "\n",
       "              created_utc                                               body  \\\n",
       "0     2025-07-28 01:14:54  they dont make diverse utopian friend groups s...   \n",
       "1     2025-07-28 06:07:07  lol i wonder what the prompt was like have stu...   \n",
       "2     2025-07-28 00:35:56  classic university college the cheapest and la...   \n",
       "3     2025-07-28 13:00:11  and now its gone httpswwwucutorontocaprogramss...   \n",
       "4     2025-07-28 04:36:00                                           yikes uc   \n",
       "...                   ...                                                ...   \n",
       "1587  2025-08-05 22:46:02  reason i thought it was scam cuz i already did...   \n",
       "1588  2025-08-05 22:54:44                                perhaps you misread   \n",
       "1589  2025-08-05 23:15:36  all students are required to enroll with duo b...   \n",
       "1590  2025-08-05 23:30:46      double check the first comment you replied to   \n",
       "1591  2025-08-05 23:32:53                                         duo is mfa   \n",
       "\n",
       "         type  score sentiment_prediction  \\\n",
       "0     comment    316                  NEG   \n",
       "1     comment    116                  NEU   \n",
       "2     comment    115                  NEG   \n",
       "3     comment     16                  NEU   \n",
       "4     comment     14                  NEG   \n",
       "...       ...    ...                  ...   \n",
       "1587  comment      3                  NEG   \n",
       "1588  comment     11                  NEU   \n",
       "1589  comment     -9                  NEU   \n",
       "1590  comment     11                  NEU   \n",
       "1591  comment      7                  NEU   \n",
       "\n",
       "                                         sentiment_prob emotion_prediction  \\\n",
       "0     {'NEG': 0.7034717202186584, 'NEU': 0.291085332...            neutral   \n",
       "1     {'NEG': 0.16222292184829712, 'NEU': 0.65833210...            neutral   \n",
       "2     {'NEG': 0.8319604992866516, 'NEU': 0.161235094...            neutral   \n",
       "3     {'NEG': 0.3041565418243408, 'NEU': 0.678175747...            neutral   \n",
       "4     {'NEG': 0.7929663062095642, 'NEU': 0.183752104...            neutral   \n",
       "...                                                 ...                ...   \n",
       "1587  {'NEG': 0.5585876107215881, 'NEU': 0.434544444...            neutral   \n",
       "1588  {'NEG': 0.3851972818374634, 'NEU': 0.596996665...            neutral   \n",
       "1589  {'NEG': 0.09463470429182053, 'NEU': 0.89856177...            neutral   \n",
       "1590  {'NEG': 0.012451458722352982, 'NEU': 0.9616963...            neutral   \n",
       "1591  {'NEG': 0.012019496411085129, 'NEU': 0.9482759...            neutral   \n",
       "\n",
       "                         emotion_prob        source_file  \n",
       "0     {'neutral': 0.6760635375976562}   submission_0.csv  \n",
       "1     {'neutral': 0.9531378149986267}   submission_0.csv  \n",
       "2     {'neutral': 0.9534212350845337}   submission_0.csv  \n",
       "3     {'neutral': 0.9478220343589783}   submission_0.csv  \n",
       "4     {'neutral': 0.9800550937652588}   submission_0.csv  \n",
       "...                               ...                ...  \n",
       "1587  {'neutral': 0.8371781706809998}  submission_99.csv  \n",
       "1588   {'neutral': 0.851717472076416}  submission_99.csv  \n",
       "1589  {'neutral': 0.9491429328918457}  submission_99.csv  \n",
       "1590  {'neutral': 0.9694627523422241}  submission_99.csv  \n",
       "1591  {'neutral': 0.9703196883201599}  submission_99.csv  \n",
       "\n",
       "[1592 rows x 13 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e941a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_counts = processed_monthly_data['sentiment_prediction'].value_counts()\n",
    "emotion_counts = processed_monthly_data['emotion_prediction'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebdea175",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_monthly_data.to_csv('processed_monthly_submissions_with_sentiment_emotion.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f865dc13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion_prediction\n",
       "neutral     1171\n",
       "joy          159\n",
       "sadness      111\n",
       "anger         81\n",
       "surprise      36\n",
       "fear          20\n",
       "disgust       14\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emotion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a730e8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_monthly_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88db87fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uoft_senti_db",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
